{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cJPUi8TnnSp2NOwFDsKOliM8nNXEqZ4Z",
      "authorship_tag": "ABX9TyN60KWLDJ4vXlE3t8n6+8Wz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OnlyourMiracle/MachineLearning/blob/master/Course/PythonMachineLearning/PythonMachineLerning_U16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0x_c5yPcujk",
        "outputId": "63daa84c-478e-4248-80e4-39de5df736de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-16 12:42:34--  https://github.com/OnlyourMiracle/Python-Machine-Learning-Second-Edition/blob/master/Chapter16/pg2265.txt\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘/content/drive/MyDrive/MLIA/Data/pg2265.txt’\n",
            "\n",
            "pg2265.txt              [ <=>                ]   1.47M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-11-16 12:42:34 (27.4 MB/s) - ‘/content/drive/MyDrive/MLIA/Data/pg2265.txt’ saved [1539648]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -P /content/drive/MyDrive/MLIA/Data https://github.com/OnlyourMiracle/Python-Machine-Learning-Second-Edition/blob/master/Chapter16/movie_data.csv.gz\n",
        "!wget -P /content/drive/MyDrive/MLIA/Data https://github.com/OnlyourMiracle/Python-Machine-Learning-Second-Edition/blob/master/Chapter16/pg2265.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUXIi27yrCCH",
        "outputId": "c7170520-2893-4a84-b3d0-141b30cd1a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyprind\n",
        "import pyprind\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/MLIA/Data/movie_data.csv')\n",
        "print(df.head(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OYrdcVxi9LM",
        "outputId": "6193c1e4-82cb-458c-88c1-7b13479d069d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n",
            "                                              review  sentiment\n",
            "0  I am surprised that there is confusion over th...          1\n",
            "1  Had I known to what I was submitting myself, I...          0\n",
            "2  i didn't enjoy this movie at all.for one,i jus...          0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing the data: separate words and count each word's occurrence\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "counts = Counter()\n",
        "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurences')\n",
        "\n",
        "for i, review in enumerate(df['review']):\n",
        "  text = ''.join([c if c not in punctuation else ' '+c+' ' for c in review]).lower()\n",
        "  df.loc[i, 'review'] = text\n",
        "  pbar.update()\n",
        "  counts.update(text.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zgd8rPMjuUj",
        "outputId": "ce791bec-e81b-4a6b-8d79-eccd2b2024aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Counting words occurences\n",
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:01:45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a mapping: Map each unique word to an integer\n",
        "\n",
        "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
        "print(word_counts[:5])\n",
        "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
        "\n",
        "mapped_reviews = []\n",
        "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
        "\n",
        "for review in df['review']:\n",
        "  mapped_reviews.append([word_to_int[word] for word in review.split()])\n",
        "  pbar.update()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96nAochTlVnn",
        "outputId": "0098755a-0614-47fc-a65f-21de340bf9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map reviews to ints\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', '.', ',', 'and', 'a']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define fixed-length sequences: use the last 200 elements of each sequence. if sequence length < 200: left-pad with zeros\n",
        "sequence_length = 200\n",
        "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
        "for i, row in enumerate(mapped_reviews):\n",
        "  review_arr = np.array(row)\n",
        "  sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
        "\n",
        "x_train = sequences[:25000, :]\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "x_test = sequences[25000:, :]\n",
        "y_test = df.loc[25000:, 'sentiment'].values\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "#function to generate minibatches\n",
        "def create_batch_generator(x, y=None, batch_size=64):\n",
        "  n_batches = len(x)//batch_size \n",
        "  x = x[:n_batches*batch_size]\n",
        "  if y is not None:\n",
        "    y = y[:n_batches*batch_size]\n",
        "  for ii in range(0, len(x), batch_size):\n",
        "    if y is not None:\n",
        "      yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
        "    else:\n",
        "      yield x[ii:ii+batch_size]"
      ],
      "metadata": {
        "id": "28AHnC1PmwiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(object):\n",
        "  def __init__(self, n_words, seq_len=200, lstm_size=256, num_layers=1, batch_size=64, learning_rate=0.0001, embed_size=200):\n",
        "    self.n_words = n_words\n",
        "    self.seq_len = seq_len\n",
        "    self.lstm_size = lstm_size\n",
        "    self.num_layers = num_layers\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.embed_size = embed_size \n",
        "\n",
        "    self.g = tf.Graph()\n",
        "    with self.g.as_default():\n",
        "      tf.set_random_seed= 123\n",
        "      self.build()\n",
        "      self.saver = tf.train.Saver()\n",
        "      self.init_op = tf.global_variables_initializer()\n",
        "\n",
        "  def build(self):\n",
        "    #define the placeholder\n",
        "    tf_x = tf.placeholder(tf.int32, shape=(self.batch_size, self.seq_len), name='tf_x')\n",
        "    tf_y = tf.placeholder(tf.float32, shape=(self.batch_size), name='tf_y')\n",
        "    tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
        "\n",
        "    #create the embedding layer\n",
        "    embedding = tf.Variable(tf.random_uniform((self.n_words, self.embed_size), minval=-1, maxval=1), name='embedding')\n",
        "    embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embeded_x')\n",
        "\n",
        "    #define LSTM cell and stack them together\n",
        "    cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell([tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.lstm_size), output_keep_prob=tf_keepprob) for i in range(self.num_layers)])\n",
        "\n",
        "    #define the initial state:\n",
        "    self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
        "    print(' << initial state>> ', self.initial_state)\n",
        "    lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cells, embed_x, initial_state=self.initial_state)\n",
        "\n",
        "    #note:lstm_outputs shape:[batch_size, max_time, cells.output_size]\n",
        "    print('\\n << lstm output >> ', lstm_outputs)\n",
        "    print('\\n << final state >> ', self.final_state)\n",
        "\n",
        "    #apply as fc layer after on top of RNN output:\n",
        "    logits = tf.layers.dense(inputs=lstm_outputs[:, -1], units=1, activation=None, name='logits')\n",
        "    logits = tf.squeeze(logits, name='logits_squeezed')\n",
        "    print('\\n << logits  >> ', logits)\n",
        "\n",
        "    y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
        "    predictions = {\n",
        "        'probabilities': y_proba, \n",
        "        'labels':tf.cast(tf.round(y_proba), tf.int32, name='labels')\n",
        "    }\n",
        "    print('\\m << predictions  >> ', predictions)\n",
        "\n",
        "    #define the cost function\n",
        "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf_y, logits=logits), name='cost')\n",
        "\n",
        "    #define the optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "    train_op = optimizer.minimize(cost, name='train_op')\n",
        "  \n",
        "  def train(self, x_train, y_train, num_epochs):\n",
        "    with tf.Session(graph=self.g) as sess:\n",
        "      sess.run(self.init_op)\n",
        "      iteration = 1\n",
        "      for epoch in range(num_epochs):\n",
        "        state = sess.run(self.initial_state)\n",
        "        for batch_x, batch_y in create_batch_generator(x_train, y_train, self.batch_size):\n",
        "          feed = {'tf_x:0':batch_x, 'tf_y:0':batch_y, 'tf_keepprob:0':0.5, self.initial_state:state}\n",
        "          loss, _ , state = sess.run(['cost:0', 'train_op', self.final_state], feed_dict=feed)\n",
        "\n",
        "          if iteration % 20 == 0:\n",
        "            print(\"Epoch: %d/%d Iteration: %d \"\"| Train loss: %.5f\" % (epoch + 1, num_epochs, iteration, loss))\n",
        "          \n",
        "          iteration += 1\n",
        "        if (epoch+1) % 10 == 0:\n",
        "          self.saver.save(sess, \"model/sentiment-%d.ckpt\" % epoch)\n",
        "\n",
        "  def predict(self, x_data, return_prob=False):\n",
        "    preds = []\n",
        "    with tf.Session(graph=self.g) as sess:\n",
        "      self.saver.restore(sess, tf.train.latest_checkpoint('model/'))\n",
        "      test_state = sess.run(self.initial_state)\n",
        "      for ii, batch_x in enumerate(create_batch_generator(x_data, None, batch_size=self.batch_size), 1):\n",
        "        feed = {'tf_x:0':batch_x, 'tf_keepprob:0': 1.0, self.initial_state:test_state}\n",
        "        if return_prob:\n",
        "          pred, test_state = sess.run(['probabilities:0', self.final_state], feed_dict=feed)\n",
        "        else:\n",
        "          pred, test_state = sess.run(['labels:0', self.final_state], feed_dict=feed)\n",
        "        preds.append(pred)\n",
        "\n",
        "    return np.concatenate(preds)\n"
      ],
      "metadata": {
        "id": "vxx0rptspdVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build a SentimentRNN example\n",
        "\n",
        "n_words = max(list(word_to_int.values())) + 1\n",
        "rnn = SentimentRNN(n_words=n_words, seq_len=sequence_length, embed_size=256, lstm_size=128, num_layers=1, batch_size=100, learning_rate=0.001)\n",
        "\n",
        "#train\n",
        "rnn.train(x_train, y_train, num_epochs=40)\n",
        "\n",
        "#predict\n",
        "preds = rnn.predict(x_test)\n",
        "y_true = y_test[:len(preds)]\n",
        "print('Test Acc: %.3f' % (np.sum(preds==y_true) / len(y_true)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTulmFbC1Cyo",
        "outputId": "1bce0ae5-b135-4ee3-d22b-33138ed0fa08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-6-4cdb3698b1b2>:34: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " << initial state>>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
            "\n",
            " << lstm output >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
            "\n",
            " << final state >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(100, 128) dtype=float32>),)\n",
            "\n",
            " << logits  >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
            "\\m << predictions  >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n",
            "Epoch: 1/40 Iteration: 20 | Train loss: 0.68579\n",
            "Epoch: 1/40 Iteration: 40 | Train loss: 0.65783\n",
            "Epoch: 1/40 Iteration: 60 | Train loss: 0.65085\n",
            "Epoch: 1/40 Iteration: 80 | Train loss: 0.65397\n",
            "Epoch: 1/40 Iteration: 100 | Train loss: 0.60502\n",
            "Epoch: 1/40 Iteration: 120 | Train loss: 0.57272\n",
            "Epoch: 1/40 Iteration: 140 | Train loss: 0.64727\n",
            "Epoch: 1/40 Iteration: 160 | Train loss: 0.50660\n",
            "Epoch: 1/40 Iteration: 180 | Train loss: 0.38954\n",
            "Epoch: 1/40 Iteration: 200 | Train loss: 0.37684\n",
            "Epoch: 1/40 Iteration: 220 | Train loss: 0.41781\n",
            "Epoch: 1/40 Iteration: 240 | Train loss: 0.47803\n",
            "Epoch: 2/40 Iteration: 260 | Train loss: 0.46846\n",
            "Epoch: 2/40 Iteration: 280 | Train loss: 0.45396\n",
            "Epoch: 2/40 Iteration: 300 | Train loss: 0.44502\n",
            "Epoch: 2/40 Iteration: 320 | Train loss: 0.54638\n",
            "Epoch: 2/40 Iteration: 340 | Train loss: 0.45043\n",
            "Epoch: 2/40 Iteration: 360 | Train loss: 0.42851\n",
            "Epoch: 2/40 Iteration: 380 | Train loss: 0.30158\n",
            "Epoch: 2/40 Iteration: 400 | Train loss: 0.51549\n",
            "Epoch: 2/40 Iteration: 420 | Train loss: 0.25728\n",
            "Epoch: 2/40 Iteration: 440 | Train loss: 0.41948\n",
            "Epoch: 2/40 Iteration: 460 | Train loss: 0.58874\n",
            "Epoch: 2/40 Iteration: 480 | Train loss: 0.35827\n",
            "Epoch: 2/40 Iteration: 500 | Train loss: 0.41119\n",
            "Epoch: 3/40 Iteration: 520 | Train loss: 0.25312\n",
            "Epoch: 3/40 Iteration: 540 | Train loss: 0.30246\n",
            "Epoch: 3/40 Iteration: 560 | Train loss: 0.32535\n",
            "Epoch: 3/40 Iteration: 580 | Train loss: 0.29034\n",
            "Epoch: 3/40 Iteration: 600 | Train loss: 0.29218\n",
            "Epoch: 3/40 Iteration: 620 | Train loss: 0.28816\n",
            "Epoch: 3/40 Iteration: 640 | Train loss: 0.30956\n",
            "Epoch: 3/40 Iteration: 660 | Train loss: 0.28423\n",
            "Epoch: 3/40 Iteration: 680 | Train loss: 0.16907\n",
            "Epoch: 3/40 Iteration: 700 | Train loss: 0.17450\n",
            "Epoch: 3/40 Iteration: 720 | Train loss: 0.17216\n",
            "Epoch: 3/40 Iteration: 740 | Train loss: 0.24600\n",
            "Epoch: 4/40 Iteration: 760 | Train loss: 0.29201\n",
            "Epoch: 4/40 Iteration: 780 | Train loss: 0.20377\n",
            "Epoch: 4/40 Iteration: 800 | Train loss: 0.12284\n",
            "Epoch: 4/40 Iteration: 820 | Train loss: 0.21994\n",
            "Epoch: 4/40 Iteration: 840 | Train loss: 0.22892\n",
            "Epoch: 4/40 Iteration: 860 | Train loss: 0.17175\n",
            "Epoch: 4/40 Iteration: 880 | Train loss: 0.24601\n",
            "Epoch: 4/40 Iteration: 900 | Train loss: 0.27826\n",
            "Epoch: 4/40 Iteration: 920 | Train loss: 0.14303\n",
            "Epoch: 4/40 Iteration: 940 | Train loss: 0.21424\n",
            "Epoch: 4/40 Iteration: 960 | Train loss: 0.21326\n",
            "Epoch: 4/40 Iteration: 980 | Train loss: 0.16180\n",
            "Epoch: 4/40 Iteration: 1000 | Train loss: 0.12917\n",
            "Epoch: 5/40 Iteration: 1020 | Train loss: 0.07731\n",
            "Epoch: 5/40 Iteration: 1040 | Train loss: 0.13298\n",
            "Epoch: 5/40 Iteration: 1060 | Train loss: 0.16539\n",
            "Epoch: 5/40 Iteration: 1080 | Train loss: 0.08952\n",
            "Epoch: 5/40 Iteration: 1100 | Train loss: 0.05936\n",
            "Epoch: 5/40 Iteration: 1120 | Train loss: 0.12258\n",
            "Epoch: 5/40 Iteration: 1140 | Train loss: 0.10367\n",
            "Epoch: 5/40 Iteration: 1160 | Train loss: 0.16152\n",
            "Epoch: 5/40 Iteration: 1180 | Train loss: 0.07946\n",
            "Epoch: 5/40 Iteration: 1200 | Train loss: 0.13733\n",
            "Epoch: 5/40 Iteration: 1220 | Train loss: 0.07806\n",
            "Epoch: 5/40 Iteration: 1240 | Train loss: 0.26028\n",
            "Epoch: 6/40 Iteration: 1260 | Train loss: 0.18319\n",
            "Epoch: 6/40 Iteration: 1280 | Train loss: 0.07683\n",
            "Epoch: 6/40 Iteration: 1300 | Train loss: 0.06043\n",
            "Epoch: 6/40 Iteration: 1320 | Train loss: 0.14941\n",
            "Epoch: 6/40 Iteration: 1340 | Train loss: 0.09428\n",
            "Epoch: 6/40 Iteration: 1360 | Train loss: 0.09889\n",
            "Epoch: 6/40 Iteration: 1380 | Train loss: 0.06505\n",
            "Epoch: 6/40 Iteration: 1400 | Train loss: 0.12482\n",
            "Epoch: 6/40 Iteration: 1420 | Train loss: 0.07981\n",
            "Epoch: 6/40 Iteration: 1440 | Train loss: 0.19813\n",
            "Epoch: 6/40 Iteration: 1460 | Train loss: 0.12610\n",
            "Epoch: 6/40 Iteration: 1480 | Train loss: 0.20435\n",
            "Epoch: 6/40 Iteration: 1500 | Train loss: 0.12320\n",
            "Epoch: 7/40 Iteration: 1520 | Train loss: 0.07547\n",
            "Epoch: 7/40 Iteration: 1540 | Train loss: 0.03531\n",
            "Epoch: 7/40 Iteration: 1560 | Train loss: 0.06138\n",
            "Epoch: 7/40 Iteration: 1580 | Train loss: 0.06225\n",
            "Epoch: 7/40 Iteration: 1600 | Train loss: 0.04183\n",
            "Epoch: 7/40 Iteration: 1620 | Train loss: 0.06631\n",
            "Epoch: 7/40 Iteration: 1640 | Train loss: 0.06894\n",
            "Epoch: 7/40 Iteration: 1660 | Train loss: 0.09222\n",
            "Epoch: 7/40 Iteration: 1680 | Train loss: 0.03737\n",
            "Epoch: 7/40 Iteration: 1700 | Train loss: 0.13336\n",
            "Epoch: 7/40 Iteration: 1720 | Train loss: 0.03360\n",
            "Epoch: 7/40 Iteration: 1740 | Train loss: 0.12240\n",
            "Epoch: 8/40 Iteration: 1760 | Train loss: 0.07073\n",
            "Epoch: 8/40 Iteration: 1780 | Train loss: 0.03231\n",
            "Epoch: 8/40 Iteration: 1800 | Train loss: 0.00943\n",
            "Epoch: 8/40 Iteration: 1820 | Train loss: 0.02160\n",
            "Epoch: 8/40 Iteration: 1840 | Train loss: 0.05628\n",
            "Epoch: 8/40 Iteration: 1860 | Train loss: 0.05075\n",
            "Epoch: 8/40 Iteration: 1880 | Train loss: 0.04919\n",
            "Epoch: 8/40 Iteration: 1900 | Train loss: 0.06177\n",
            "Epoch: 8/40 Iteration: 1920 | Train loss: 0.01517\n",
            "Epoch: 8/40 Iteration: 1940 | Train loss: 0.01867\n",
            "Epoch: 8/40 Iteration: 1960 | Train loss: 0.05488\n",
            "Epoch: 8/40 Iteration: 1980 | Train loss: 0.05722\n",
            "Epoch: 8/40 Iteration: 2000 | Train loss: 0.00996\n",
            "Epoch: 9/40 Iteration: 2020 | Train loss: 0.01431\n",
            "Epoch: 9/40 Iteration: 2040 | Train loss: 0.13976\n",
            "Epoch: 9/40 Iteration: 2060 | Train loss: 0.04179\n",
            "Epoch: 9/40 Iteration: 2080 | Train loss: 0.03398\n",
            "Epoch: 9/40 Iteration: 2100 | Train loss: 0.04961\n",
            "Epoch: 9/40 Iteration: 2120 | Train loss: 0.16365\n",
            "Epoch: 9/40 Iteration: 2140 | Train loss: 0.03661\n",
            "Epoch: 9/40 Iteration: 2160 | Train loss: 0.02765\n",
            "Epoch: 9/40 Iteration: 2180 | Train loss: 0.01166\n",
            "Epoch: 9/40 Iteration: 2200 | Train loss: 0.08284\n",
            "Epoch: 9/40 Iteration: 2220 | Train loss: 0.01792\n",
            "Epoch: 9/40 Iteration: 2240 | Train loss: 0.09224\n",
            "Epoch: 10/40 Iteration: 2260 | Train loss: 0.02021\n",
            "Epoch: 10/40 Iteration: 2280 | Train loss: 0.01597\n",
            "Epoch: 10/40 Iteration: 2300 | Train loss: 0.02408\n",
            "Epoch: 10/40 Iteration: 2320 | Train loss: 0.03660\n",
            "Epoch: 10/40 Iteration: 2340 | Train loss: 0.00690\n",
            "Epoch: 10/40 Iteration: 2360 | Train loss: 0.07386\n",
            "Epoch: 10/40 Iteration: 2380 | Train loss: 0.01072\n",
            "Epoch: 10/40 Iteration: 2400 | Train loss: 0.07477\n",
            "Epoch: 10/40 Iteration: 2420 | Train loss: 0.01423\n",
            "Epoch: 10/40 Iteration: 2440 | Train loss: 0.00957\n",
            "Epoch: 10/40 Iteration: 2460 | Train loss: 0.01083\n",
            "Epoch: 10/40 Iteration: 2480 | Train loss: 0.03488\n",
            "Epoch: 10/40 Iteration: 2500 | Train loss: 0.00311\n",
            "Epoch: 11/40 Iteration: 2520 | Train loss: 0.01334\n",
            "Epoch: 11/40 Iteration: 2540 | Train loss: 0.00538\n",
            "Epoch: 11/40 Iteration: 2560 | Train loss: 0.02597\n",
            "Epoch: 11/40 Iteration: 2580 | Train loss: 0.01048\n",
            "Epoch: 11/40 Iteration: 2600 | Train loss: 0.00744\n",
            "Epoch: 11/40 Iteration: 2620 | Train loss: 0.00268\n",
            "Epoch: 11/40 Iteration: 2640 | Train loss: 0.00755\n",
            "Epoch: 11/40 Iteration: 2660 | Train loss: 0.00429\n",
            "Epoch: 11/40 Iteration: 2680 | Train loss: 0.02525\n",
            "Epoch: 11/40 Iteration: 2700 | Train loss: 0.09343\n",
            "Epoch: 11/40 Iteration: 2720 | Train loss: 0.06861\n",
            "Epoch: 11/40 Iteration: 2740 | Train loss: 0.00606\n",
            "Epoch: 12/40 Iteration: 2760 | Train loss: 0.00815\n",
            "Epoch: 12/40 Iteration: 2780 | Train loss: 0.01080\n",
            "Epoch: 12/40 Iteration: 2800 | Train loss: 0.00505\n",
            "Epoch: 12/40 Iteration: 2820 | Train loss: 0.04110\n",
            "Epoch: 12/40 Iteration: 2840 | Train loss: 0.03323\n",
            "Epoch: 12/40 Iteration: 2860 | Train loss: 0.01112\n",
            "Epoch: 12/40 Iteration: 2880 | Train loss: 0.00672\n",
            "Epoch: 12/40 Iteration: 2900 | Train loss: 0.00348\n",
            "Epoch: 12/40 Iteration: 2920 | Train loss: 0.00474\n",
            "Epoch: 12/40 Iteration: 2940 | Train loss: 0.00206\n",
            "Epoch: 12/40 Iteration: 2960 | Train loss: 0.01549\n",
            "Epoch: 12/40 Iteration: 2980 | Train loss: 0.00174\n",
            "Epoch: 12/40 Iteration: 3000 | Train loss: 0.00245\n",
            "Epoch: 13/40 Iteration: 3020 | Train loss: 0.00380\n",
            "Epoch: 13/40 Iteration: 3040 | Train loss: 0.00174\n",
            "Epoch: 13/40 Iteration: 3060 | Train loss: 0.03917\n",
            "Epoch: 13/40 Iteration: 3080 | Train loss: 0.01103\n",
            "Epoch: 13/40 Iteration: 3100 | Train loss: 0.01384\n",
            "Epoch: 13/40 Iteration: 3120 | Train loss: 0.00334\n",
            "Epoch: 13/40 Iteration: 3140 | Train loss: 0.01010\n",
            "Epoch: 13/40 Iteration: 3160 | Train loss: 0.01436\n",
            "Epoch: 13/40 Iteration: 3180 | Train loss: 0.00660\n",
            "Epoch: 13/40 Iteration: 3200 | Train loss: 0.04929\n",
            "Epoch: 13/40 Iteration: 3220 | Train loss: 0.00406\n",
            "Epoch: 13/40 Iteration: 3240 | Train loss: 0.02111\n",
            "Epoch: 14/40 Iteration: 3260 | Train loss: 0.00260\n",
            "Epoch: 14/40 Iteration: 3280 | Train loss: 0.00128\n",
            "Epoch: 14/40 Iteration: 3300 | Train loss: 0.02657\n",
            "Epoch: 14/40 Iteration: 3320 | Train loss: 0.00286\n",
            "Epoch: 14/40 Iteration: 3340 | Train loss: 0.00244\n",
            "Epoch: 14/40 Iteration: 3360 | Train loss: 0.02676\n",
            "Epoch: 14/40 Iteration: 3380 | Train loss: 0.01272\n",
            "Epoch: 14/40 Iteration: 3400 | Train loss: 0.00530\n",
            "Epoch: 14/40 Iteration: 3420 | Train loss: 0.00257\n",
            "Epoch: 14/40 Iteration: 3440 | Train loss: 0.01193\n",
            "Epoch: 14/40 Iteration: 3460 | Train loss: 0.01590\n",
            "Epoch: 14/40 Iteration: 3480 | Train loss: 0.05352\n",
            "Epoch: 14/40 Iteration: 3500 | Train loss: 0.00152\n",
            "Epoch: 15/40 Iteration: 3520 | Train loss: 0.00641\n",
            "Epoch: 15/40 Iteration: 3540 | Train loss: 0.00168\n",
            "Epoch: 15/40 Iteration: 3560 | Train loss: 0.00261\n",
            "Epoch: 15/40 Iteration: 3580 | Train loss: 0.01587\n",
            "Epoch: 15/40 Iteration: 3600 | Train loss: 0.00165\n",
            "Epoch: 15/40 Iteration: 3620 | Train loss: 0.00133\n",
            "Epoch: 15/40 Iteration: 3640 | Train loss: 0.00396\n",
            "Epoch: 15/40 Iteration: 3660 | Train loss: 0.00323\n",
            "Epoch: 15/40 Iteration: 3680 | Train loss: 0.00447\n",
            "Epoch: 15/40 Iteration: 3700 | Train loss: 0.02448\n",
            "Epoch: 15/40 Iteration: 3720 | Train loss: 0.00230\n",
            "Epoch: 15/40 Iteration: 3740 | Train loss: 0.00222\n",
            "Epoch: 16/40 Iteration: 3760 | Train loss: 0.00260\n",
            "Epoch: 16/40 Iteration: 3780 | Train loss: 0.01373\n",
            "Epoch: 16/40 Iteration: 3800 | Train loss: 0.00675\n",
            "Epoch: 16/40 Iteration: 3820 | Train loss: 0.00353\n",
            "Epoch: 16/40 Iteration: 3840 | Train loss: 0.00324\n",
            "Epoch: 16/40 Iteration: 3860 | Train loss: 0.00153\n",
            "Epoch: 16/40 Iteration: 3880 | Train loss: 0.00383\n",
            "Epoch: 16/40 Iteration: 3900 | Train loss: 0.02308\n",
            "Epoch: 16/40 Iteration: 3920 | Train loss: 0.01776\n",
            "Epoch: 16/40 Iteration: 3940 | Train loss: 0.18215\n",
            "Epoch: 16/40 Iteration: 3960 | Train loss: 0.05388\n",
            "Epoch: 16/40 Iteration: 3980 | Train loss: 0.00460\n",
            "Epoch: 16/40 Iteration: 4000 | Train loss: 0.00300\n",
            "Epoch: 17/40 Iteration: 4020 | Train loss: 0.00206\n",
            "Epoch: 17/40 Iteration: 4040 | Train loss: 0.00359\n",
            "Epoch: 17/40 Iteration: 4060 | Train loss: 0.00586\n",
            "Epoch: 17/40 Iteration: 4080 | Train loss: 0.00510\n",
            "Epoch: 17/40 Iteration: 4100 | Train loss: 0.01238\n",
            "Epoch: 17/40 Iteration: 4120 | Train loss: 0.00252\n",
            "Epoch: 17/40 Iteration: 4140 | Train loss: 0.00131\n",
            "Epoch: 17/40 Iteration: 4160 | Train loss: 0.00868\n",
            "Epoch: 17/40 Iteration: 4180 | Train loss: 0.00376\n",
            "Epoch: 17/40 Iteration: 4200 | Train loss: 0.03904\n",
            "Epoch: 17/40 Iteration: 4220 | Train loss: 0.00232\n",
            "Epoch: 17/40 Iteration: 4240 | Train loss: 0.01175\n",
            "Epoch: 18/40 Iteration: 4260 | Train loss: 0.00167\n",
            "Epoch: 18/40 Iteration: 4280 | Train loss: 0.00139\n",
            "Epoch: 18/40 Iteration: 4300 | Train loss: 0.02634\n",
            "Epoch: 18/40 Iteration: 4320 | Train loss: 0.00557\n",
            "Epoch: 18/40 Iteration: 4340 | Train loss: 0.00218\n",
            "Epoch: 18/40 Iteration: 4360 | Train loss: 0.00374\n",
            "Epoch: 18/40 Iteration: 4380 | Train loss: 0.00370\n",
            "Epoch: 18/40 Iteration: 4400 | Train loss: 0.02758\n",
            "Epoch: 18/40 Iteration: 4420 | Train loss: 0.00277\n",
            "Epoch: 18/40 Iteration: 4440 | Train loss: 0.03352\n",
            "Epoch: 18/40 Iteration: 4460 | Train loss: 0.00435\n",
            "Epoch: 18/40 Iteration: 4480 | Train loss: 0.00921\n",
            "Epoch: 18/40 Iteration: 4500 | Train loss: 0.00758\n",
            "Epoch: 19/40 Iteration: 4520 | Train loss: 0.00095\n",
            "Epoch: 19/40 Iteration: 4540 | Train loss: 0.00303\n",
            "Epoch: 19/40 Iteration: 4560 | Train loss: 0.00653\n",
            "Epoch: 19/40 Iteration: 4580 | Train loss: 0.00382\n",
            "Epoch: 19/40 Iteration: 4600 | Train loss: 0.00177\n",
            "Epoch: 19/40 Iteration: 4620 | Train loss: 0.00136\n",
            "Epoch: 19/40 Iteration: 4640 | Train loss: 0.04477\n",
            "Epoch: 19/40 Iteration: 4660 | Train loss: 0.03992\n",
            "Epoch: 19/40 Iteration: 4680 | Train loss: 0.00338\n",
            "Epoch: 19/40 Iteration: 4700 | Train loss: 0.00988\n",
            "Epoch: 19/40 Iteration: 4720 | Train loss: 0.00647\n",
            "Epoch: 19/40 Iteration: 4740 | Train loss: 0.00550\n",
            "Epoch: 20/40 Iteration: 4760 | Train loss: 0.01298\n",
            "Epoch: 20/40 Iteration: 4780 | Train loss: 0.01063\n",
            "Epoch: 20/40 Iteration: 4800 | Train loss: 0.00175\n",
            "Epoch: 20/40 Iteration: 4820 | Train loss: 0.00110\n",
            "Epoch: 20/40 Iteration: 4840 | Train loss: 0.00240\n",
            "Epoch: 20/40 Iteration: 4860 | Train loss: 0.00987\n",
            "Epoch: 20/40 Iteration: 4880 | Train loss: 0.01028\n",
            "Epoch: 20/40 Iteration: 4900 | Train loss: 0.00279\n",
            "Epoch: 20/40 Iteration: 4920 | Train loss: 0.00453\n",
            "Epoch: 20/40 Iteration: 4940 | Train loss: 0.00121\n",
            "Epoch: 20/40 Iteration: 4960 | Train loss: 0.05694\n",
            "Epoch: 20/40 Iteration: 4980 | Train loss: 0.00508\n",
            "Epoch: 20/40 Iteration: 5000 | Train loss: 0.00625\n",
            "Epoch: 21/40 Iteration: 5020 | Train loss: 0.00894\n",
            "Epoch: 21/40 Iteration: 5040 | Train loss: 0.00588\n",
            "Epoch: 21/40 Iteration: 5060 | Train loss: 0.00100\n",
            "Epoch: 21/40 Iteration: 5080 | Train loss: 0.00422\n",
            "Epoch: 21/40 Iteration: 5100 | Train loss: 0.00505\n",
            "Epoch: 21/40 Iteration: 5120 | Train loss: 0.00901\n",
            "Epoch: 21/40 Iteration: 5140 | Train loss: 0.00071\n",
            "Epoch: 21/40 Iteration: 5160 | Train loss: 0.00072\n",
            "Epoch: 21/40 Iteration: 5180 | Train loss: 0.00183\n",
            "Epoch: 21/40 Iteration: 5200 | Train loss: 0.18898\n",
            "Epoch: 21/40 Iteration: 5220 | Train loss: 0.00381\n",
            "Epoch: 21/40 Iteration: 5240 | Train loss: 0.02909\n",
            "Epoch: 22/40 Iteration: 5260 | Train loss: 0.03164\n",
            "Epoch: 22/40 Iteration: 5280 | Train loss: 0.00077\n",
            "Epoch: 22/40 Iteration: 5300 | Train loss: 0.00125\n",
            "Epoch: 22/40 Iteration: 5320 | Train loss: 0.00039\n",
            "Epoch: 22/40 Iteration: 5340 | Train loss: 0.00086\n",
            "Epoch: 22/40 Iteration: 5360 | Train loss: 0.00037\n",
            "Epoch: 22/40 Iteration: 5380 | Train loss: 0.00053\n",
            "Epoch: 22/40 Iteration: 5400 | Train loss: 0.00095\n",
            "Epoch: 22/40 Iteration: 5420 | Train loss: 0.00014\n",
            "Epoch: 22/40 Iteration: 5440 | Train loss: 0.00007\n",
            "Epoch: 22/40 Iteration: 5460 | Train loss: 0.00027\n",
            "Epoch: 22/40 Iteration: 5480 | Train loss: 0.00034\n",
            "Epoch: 22/40 Iteration: 5500 | Train loss: 0.00026\n",
            "Epoch: 23/40 Iteration: 5520 | Train loss: 0.00033\n",
            "Epoch: 23/40 Iteration: 5540 | Train loss: 0.00031\n",
            "Epoch: 23/40 Iteration: 5560 | Train loss: 0.00057\n",
            "Epoch: 23/40 Iteration: 5580 | Train loss: 0.00084\n",
            "Epoch: 23/40 Iteration: 5600 | Train loss: 0.00023\n",
            "Epoch: 23/40 Iteration: 5620 | Train loss: 0.00020\n",
            "Epoch: 23/40 Iteration: 5640 | Train loss: 0.00020\n",
            "Epoch: 23/40 Iteration: 5660 | Train loss: 0.00011\n",
            "Epoch: 23/40 Iteration: 5680 | Train loss: 0.00086\n",
            "Epoch: 23/40 Iteration: 5700 | Train loss: 0.00310\n",
            "Epoch: 23/40 Iteration: 5720 | Train loss: 0.00019\n",
            "Epoch: 23/40 Iteration: 5740 | Train loss: 0.00039\n",
            "Epoch: 24/40 Iteration: 5760 | Train loss: 0.00012\n",
            "Epoch: 24/40 Iteration: 5780 | Train loss: 0.00017\n",
            "Epoch: 24/40 Iteration: 5800 | Train loss: 0.00038\n",
            "Epoch: 24/40 Iteration: 5820 | Train loss: 0.00020\n",
            "Epoch: 24/40 Iteration: 5840 | Train loss: 0.00096\n",
            "Epoch: 24/40 Iteration: 5860 | Train loss: 0.00021\n",
            "Epoch: 24/40 Iteration: 5880 | Train loss: 0.00086\n",
            "Epoch: 24/40 Iteration: 5900 | Train loss: 0.00021\n",
            "Epoch: 24/40 Iteration: 5920 | Train loss: 0.00007\n",
            "Epoch: 24/40 Iteration: 5940 | Train loss: 0.00005\n",
            "Epoch: 24/40 Iteration: 5960 | Train loss: 0.00010\n",
            "Epoch: 24/40 Iteration: 5980 | Train loss: 0.00015\n",
            "Epoch: 24/40 Iteration: 6000 | Train loss: 0.00006\n",
            "Epoch: 25/40 Iteration: 6020 | Train loss: 0.00007\n",
            "Epoch: 25/40 Iteration: 6040 | Train loss: 0.00002\n",
            "Epoch: 25/40 Iteration: 6060 | Train loss: 0.00032\n",
            "Epoch: 25/40 Iteration: 6080 | Train loss: 0.00018\n",
            "Epoch: 25/40 Iteration: 6100 | Train loss: 0.00010\n",
            "Epoch: 25/40 Iteration: 6120 | Train loss: 0.00006\n",
            "Epoch: 25/40 Iteration: 6140 | Train loss: 0.00007\n",
            "Epoch: 25/40 Iteration: 6160 | Train loss: 0.00017\n",
            "Epoch: 25/40 Iteration: 6180 | Train loss: 0.00030\n",
            "Epoch: 25/40 Iteration: 6200 | Train loss: 0.00035\n",
            "Epoch: 25/40 Iteration: 6220 | Train loss: 0.00026\n",
            "Epoch: 25/40 Iteration: 6240 | Train loss: 0.00036\n",
            "Epoch: 26/40 Iteration: 6260 | Train loss: 0.00003\n",
            "Epoch: 26/40 Iteration: 6280 | Train loss: 0.00003\n",
            "Epoch: 26/40 Iteration: 6300 | Train loss: 0.00009\n",
            "Epoch: 26/40 Iteration: 6320 | Train loss: 0.00008\n",
            "Epoch: 26/40 Iteration: 6340 | Train loss: 0.00048\n",
            "Epoch: 26/40 Iteration: 6360 | Train loss: 0.00006\n",
            "Epoch: 26/40 Iteration: 6380 | Train loss: 0.00033\n",
            "Epoch: 26/40 Iteration: 6400 | Train loss: 0.00005\n",
            "Epoch: 26/40 Iteration: 6420 | Train loss: 0.00006\n",
            "Epoch: 26/40 Iteration: 6440 | Train loss: 0.00008\n",
            "Epoch: 26/40 Iteration: 6460 | Train loss: 0.00007\n",
            "Epoch: 26/40 Iteration: 6480 | Train loss: 0.00013\n",
            "Epoch: 26/40 Iteration: 6500 | Train loss: 0.00004\n",
            "Epoch: 27/40 Iteration: 6520 | Train loss: 0.00009\n",
            "Epoch: 27/40 Iteration: 6540 | Train loss: 0.00002\n",
            "Epoch: 27/40 Iteration: 6560 | Train loss: 0.00019\n",
            "Epoch: 27/40 Iteration: 6580 | Train loss: 0.00017\n",
            "Epoch: 27/40 Iteration: 6600 | Train loss: 0.00014\n",
            "Epoch: 27/40 Iteration: 6620 | Train loss: 0.00008\n",
            "Epoch: 27/40 Iteration: 6640 | Train loss: 0.00017\n",
            "Epoch: 27/40 Iteration: 6660 | Train loss: 0.00008\n",
            "Epoch: 27/40 Iteration: 6680 | Train loss: 0.00058\n",
            "Epoch: 27/40 Iteration: 6700 | Train loss: 0.01453\n",
            "Epoch: 27/40 Iteration: 6720 | Train loss: 0.00011\n",
            "Epoch: 27/40 Iteration: 6740 | Train loss: 0.00009\n",
            "Epoch: 28/40 Iteration: 6760 | Train loss: 0.00006\n",
            "Epoch: 28/40 Iteration: 6780 | Train loss: 0.00005\n",
            "Epoch: 28/40 Iteration: 6800 | Train loss: 0.00001\n",
            "Epoch: 28/40 Iteration: 6820 | Train loss: 0.00003\n",
            "Epoch: 28/40 Iteration: 6840 | Train loss: 0.00007\n",
            "Epoch: 28/40 Iteration: 6860 | Train loss: 0.00003\n",
            "Epoch: 28/40 Iteration: 6880 | Train loss: 0.00015\n",
            "Epoch: 28/40 Iteration: 6900 | Train loss: 0.00001\n",
            "Epoch: 28/40 Iteration: 6920 | Train loss: 0.00003\n",
            "Epoch: 28/40 Iteration: 6940 | Train loss: 0.00001\n",
            "Epoch: 28/40 Iteration: 6960 | Train loss: 0.00004\n",
            "Epoch: 28/40 Iteration: 6980 | Train loss: 0.00006\n",
            "Epoch: 28/40 Iteration: 7000 | Train loss: 0.00003\n",
            "Epoch: 29/40 Iteration: 7020 | Train loss: 0.00004\n",
            "Epoch: 29/40 Iteration: 7040 | Train loss: 0.00002\n",
            "Epoch: 29/40 Iteration: 7060 | Train loss: 0.00009\n",
            "Epoch: 29/40 Iteration: 7080 | Train loss: 0.00011\n",
            "Epoch: 29/40 Iteration: 7100 | Train loss: 0.00004\n",
            "Epoch: 29/40 Iteration: 7120 | Train loss: 0.00002\n",
            "Epoch: 29/40 Iteration: 7140 | Train loss: 0.00013\n",
            "Epoch: 29/40 Iteration: 7160 | Train loss: 0.00003\n",
            "Epoch: 29/40 Iteration: 7180 | Train loss: 0.00014\n",
            "Epoch: 29/40 Iteration: 7200 | Train loss: 0.00002\n",
            "Epoch: 29/40 Iteration: 7220 | Train loss: 0.00003\n",
            "Epoch: 29/40 Iteration: 7240 | Train loss: 0.00030\n",
            "Epoch: 30/40 Iteration: 7260 | Train loss: 0.00011\n",
            "Epoch: 30/40 Iteration: 7280 | Train loss: 0.00001\n",
            "Epoch: 30/40 Iteration: 7300 | Train loss: 0.00004\n",
            "Epoch: 30/40 Iteration: 7320 | Train loss: 0.00002\n",
            "Epoch: 30/40 Iteration: 7340 | Train loss: 0.00002\n",
            "Epoch: 30/40 Iteration: 7360 | Train loss: 0.00002\n",
            "Epoch: 30/40 Iteration: 7380 | Train loss: 0.00009\n",
            "Epoch: 30/40 Iteration: 7400 | Train loss: 0.00002\n",
            "Epoch: 30/40 Iteration: 7420 | Train loss: 0.00002\n",
            "Epoch: 30/40 Iteration: 7440 | Train loss: 0.00000\n",
            "Epoch: 30/40 Iteration: 7460 | Train loss: 0.00004\n",
            "Epoch: 30/40 Iteration: 7480 | Train loss: 0.00003\n",
            "Epoch: 30/40 Iteration: 7500 | Train loss: 0.00001\n",
            "Epoch: 31/40 Iteration: 7520 | Train loss: 0.00002\n",
            "Epoch: 31/40 Iteration: 7540 | Train loss: 0.00002\n",
            "Epoch: 31/40 Iteration: 7560 | Train loss: 0.00012\n",
            "Epoch: 31/40 Iteration: 7580 | Train loss: 0.00006\n",
            "Epoch: 31/40 Iteration: 7600 | Train loss: 0.00005\n",
            "Epoch: 31/40 Iteration: 7620 | Train loss: 0.00001\n",
            "Epoch: 31/40 Iteration: 7640 | Train loss: 0.00012\n",
            "Epoch: 31/40 Iteration: 7660 | Train loss: 0.00002\n",
            "Epoch: 31/40 Iteration: 7680 | Train loss: 0.00001\n",
            "Epoch: 31/40 Iteration: 7700 | Train loss: 0.00002\n",
            "Epoch: 31/40 Iteration: 7720 | Train loss: 0.00004\n",
            "Epoch: 31/40 Iteration: 7740 | Train loss: 0.00046\n",
            "Epoch: 32/40 Iteration: 7760 | Train loss: 0.00004\n",
            "Epoch: 32/40 Iteration: 7780 | Train loss: 0.00001\n",
            "Epoch: 32/40 Iteration: 7800 | Train loss: 0.00001\n",
            "Epoch: 32/40 Iteration: 7820 | Train loss: 0.00007\n",
            "Epoch: 32/40 Iteration: 7840 | Train loss: 0.00002\n",
            "Epoch: 32/40 Iteration: 7860 | Train loss: 0.00001\n",
            "Epoch: 32/40 Iteration: 7880 | Train loss: 0.00004\n",
            "Epoch: 32/40 Iteration: 7900 | Train loss: 0.00001\n",
            "Epoch: 32/40 Iteration: 7920 | Train loss: 0.00001\n",
            "Epoch: 32/40 Iteration: 7940 | Train loss: 0.00000\n",
            "Epoch: 32/40 Iteration: 7960 | Train loss: 0.00004\n",
            "Epoch: 32/40 Iteration: 7980 | Train loss: 0.00002\n",
            "Epoch: 32/40 Iteration: 8000 | Train loss: 0.00001\n",
            "Epoch: 33/40 Iteration: 8020 | Train loss: 0.00001\n",
            "Epoch: 33/40 Iteration: 8040 | Train loss: 0.00000\n",
            "Epoch: 33/40 Iteration: 8060 | Train loss: 0.00006\n",
            "Epoch: 33/40 Iteration: 8080 | Train loss: 0.00006\n",
            "Epoch: 33/40 Iteration: 8100 | Train loss: 0.00013\n",
            "Epoch: 33/40 Iteration: 8120 | Train loss: 0.00001\n",
            "Epoch: 33/40 Iteration: 8140 | Train loss: 0.00006\n",
            "Epoch: 33/40 Iteration: 8160 | Train loss: 0.00004\n",
            "Epoch: 33/40 Iteration: 8180 | Train loss: 0.00005\n",
            "Epoch: 33/40 Iteration: 8200 | Train loss: 0.00001\n",
            "Epoch: 33/40 Iteration: 8220 | Train loss: 0.00003\n",
            "Epoch: 33/40 Iteration: 8240 | Train loss: 0.00007\n",
            "Epoch: 34/40 Iteration: 8260 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8280 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8300 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8320 | Train loss: 0.00002\n",
            "Epoch: 34/40 Iteration: 8340 | Train loss: 0.00002\n",
            "Epoch: 34/40 Iteration: 8360 | Train loss: 0.00003\n",
            "Epoch: 34/40 Iteration: 8380 | Train loss: 0.00008\n",
            "Epoch: 34/40 Iteration: 8400 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8420 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8440 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8460 | Train loss: 0.00003\n",
            "Epoch: 34/40 Iteration: 8480 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8500 | Train loss: 0.00002\n",
            "Epoch: 35/40 Iteration: 8520 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8540 | Train loss: 0.00000\n",
            "Epoch: 35/40 Iteration: 8560 | Train loss: 0.00003\n",
            "Epoch: 35/40 Iteration: 8580 | Train loss: 0.00004\n",
            "Epoch: 35/40 Iteration: 8600 | Train loss: 0.00005\n",
            "Epoch: 35/40 Iteration: 8620 | Train loss: 0.00000\n",
            "Epoch: 35/40 Iteration: 8640 | Train loss: 0.00004\n",
            "Epoch: 35/40 Iteration: 8660 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8680 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8700 | Train loss: 0.00000\n",
            "Epoch: 35/40 Iteration: 8720 | Train loss: 0.00003\n",
            "Epoch: 35/40 Iteration: 8740 | Train loss: 0.00014\n",
            "Epoch: 36/40 Iteration: 8760 | Train loss: 0.00000\n",
            "Epoch: 36/40 Iteration: 8780 | Train loss: 0.00000\n",
            "Epoch: 36/40 Iteration: 8800 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8820 | Train loss: 0.00003\n",
            "Epoch: 36/40 Iteration: 8840 | Train loss: 0.00006\n",
            "Epoch: 36/40 Iteration: 8860 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8880 | Train loss: 0.00012\n",
            "Epoch: 36/40 Iteration: 8900 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8920 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8940 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8960 | Train loss: 0.00000\n",
            "Epoch: 36/40 Iteration: 8980 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 9000 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9020 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9040 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9060 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9080 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9100 | Train loss: 0.00016\n",
            "Epoch: 37/40 Iteration: 9120 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9140 | Train loss: 0.00002\n",
            "Epoch: 37/40 Iteration: 9160 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9180 | Train loss: 0.00002\n",
            "Epoch: 37/40 Iteration: 9200 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9220 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9240 | Train loss: 0.00046\n",
            "Epoch: 38/40 Iteration: 9260 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9280 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9300 | Train loss: 0.00002\n",
            "Epoch: 38/40 Iteration: 9320 | Train loss: 0.00003\n",
            "Epoch: 38/40 Iteration: 9340 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9360 | Train loss: 0.00000\n",
            "Epoch: 38/40 Iteration: 9380 | Train loss: 0.00003\n",
            "Epoch: 38/40 Iteration: 9400 | Train loss: 0.00000\n",
            "Epoch: 38/40 Iteration: 9420 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9440 | Train loss: 0.00000\n",
            "Epoch: 38/40 Iteration: 9460 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9480 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9500 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9520 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9540 | Train loss: 0.00002\n",
            "Epoch: 39/40 Iteration: 9560 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9580 | Train loss: 0.00007\n",
            "Epoch: 39/40 Iteration: 9600 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9620 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9640 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9660 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9680 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9700 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9720 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9740 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9760 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9780 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9800 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9820 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9840 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9860 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9880 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9900 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9920 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9940 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9960 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9980 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 10000 | Train loss: 0.00001\n",
            "Test Acc: 0.853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo curl -L http://www.gutenberg.org/cache/epub/2265/pg2265.txt -o /content/drive/MyDrive/MLIA/Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zApxOi9aLhXb",
        "outputId": "a4ee8c8b-4af2-4bd2-bd6d-9089410f74af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "Warning: Failed to create the file /content/drive/MyDrive/MLIA/Data: Is a \n",
            "Warning: directory\n",
            "\r  0  180k    0  1139    0     0   9112      0  0:00:20 --:--:--  0:00:20  9112\n",
            "curl: (23) Failed writing body (0 != 1139)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Reading and processing text\n",
        "with open('/content/drive/MyDrive/MLIA/Data/pg2265.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "text = text[15858:]\n",
        "chars = set(text)\n",
        "char2int = {ch:i for i, ch in enumerate(chars)}\n",
        "int2char = dict(enumerate(chars))\n",
        "text_ints = np.array([char2int[ch] for ch in text], dtype=np.int32)"
      ],
      "metadata": {
        "id": "pe3jW7sSNLgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_data(sequence, batch_size, num_steps):\n",
        "  tot_batch_length = batch_size * num_steps\n",
        "  num_batches = int(len(sequence) / tot_batch_length)\n",
        "  if num_batches*tot_batch_length + 1 > len(sequence):\n",
        "    num_batches = num_batches - 1\n",
        "  \n",
        "  #truncate the sequence at the end to get rid of remaining charcaters that do not make a full batch\n",
        "  x = sequence[0:num_batches*tot_batch_length]\n",
        "  y = sequence[1:num_batches*tot_batch_length + 1]\n",
        "\n",
        "  #split x & y into a list batches of sequences\n",
        "  x_batch_splits = np.split(x, batch_size)\n",
        "  y_batch_splits = np.split(y, batch_size)\n",
        "\n",
        "  #Stack the batches together\n",
        "  #batch_size x tot_batch_length\n",
        "  x = np.stack(x_batch_splits)\n",
        "  y = np.stack(y_batch_splits)\n",
        "\n",
        "  return x, y \n",
        "\n",
        "#Testing\n",
        "train_x, train_y = reshape_data(text_ints, 64, 10)\n",
        "print(train_x.shape)\n",
        "print(train_x[0, :10])\n",
        "print(train_y[0, :10])\n",
        "print(''.join(int2char[i] for i in train_x[0, :50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6G662CCOC4Y",
        "outputId": "a76e5017-dc2e-465f-dcf6-bb892aad01ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 23800)\n",
            "[87 12 69 92 34 16 64 25 43 84]\n",
            "[12 69 92 34 16 64 25 43 84 12]\n",
            " &quot;key&quot;: &quot;click.take_survey&quot;}, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(123)\n",
        "\n",
        "def create_batch_generator(data_x, data_y, num_steps):\n",
        "  batch_size, tot_batch_length = data_x.shape\n",
        "  num_batches = int(tot_batch_length/num_steps)\n",
        "  for b in range(num_batches):\n",
        "    yield (data_x[:, b*num_steps:(b+1)*num_steps],\n",
        "           data_y[:, b*num_steps:(b+1)*num_steps])\n",
        "\n",
        "bgen = create_batch_generator(train_x[:, :100], train_y[:, :100], 15)\n",
        "for b in bgen:\n",
        "  print(b[0].shape, b[1].shape, end=' ')\n",
        "  print(''.join(int2char[i] for i in b[0][0, :]).replace('\\n', '*'), ' ',\n",
        "        ''.join(int2char[i] for i in b[1][0, :]).replace('\\n', '*'))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2Zk0w19RIZA",
        "outputId": "db2429bb-e4c8-473b-d921-30cfa6bec5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 15) (64, 15)  &quot;key&quot   &quot;key&quot;\n",
            "(64, 15) (64, 15) ;: &quot;click.   : &quot;click.t\n",
            "(64, 15) (64, 15) take_survey&quo   ake_survey&quot\n",
            "(64, 15) (64, 15) t;}, {&quot;exp   ;}, {&quot;expe\n",
            "(64, 15) (64, 15) erimentIds&quot   rimentIds&quot;\n",
            "(64, 15) (64, 15) ;: [], &quot;id   : [], &quot;id&\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build the character-levell RNN model\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import os\n",
        "\n",
        "class CharRNN(object):\n",
        "  def __init__(self, num_classes, batch_size=64, num_steps=100, lstm_size=128, num_layers=1, learning_rate=0.001, keep_prob=0.5, grad_clip=5, \n",
        "               sampling=False):\n",
        "    self.num_classes = num_classes\n",
        "    self.batch_size = batch_size\n",
        "    self.lstm_size = lstm_size\n",
        "    self.num_layers = num_layers \n",
        "    self.learning_rate = learning_rate\n",
        "    self.keep_prob = keep_prob\n",
        "    self.grad_clip = grad_clip\n",
        "    self.num_steps = num_steps\n",
        "\n",
        "    self.g = tf.Graph()\n",
        "    with self.g.as_default():\n",
        "      tf.set_random_seed(123)\n",
        "\n",
        "      self.build(sampling=sampling)\n",
        "      self.saver = tf.train.Saver()\n",
        "      self.init_op = tf.global_variables_initializer()\n",
        "\n",
        "  def build(self, sampling):\n",
        "    if sampling == True:\n",
        "      batch_size, num_steps = 1, 1\n",
        "    else:\n",
        "      batch_size = self.batch_size\n",
        "      num_steps = self.num_steps\n",
        "    \n",
        "    tf_x = tf.placeholder(tf.int32, shape=[batch_size, num_steps], name='tf_x')\n",
        "    tf_y = tf.placeholder(tf.int32, shape=[batch_size, num_steps], name='tf_y')\n",
        "    tf_keepprob = tf.placeholder(tf.float32, name='tf_keepprob')\n",
        "\n",
        "    #one-hot encoding\n",
        "    x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
        "    y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
        "\n",
        "    #build the multi-layer RNN cells\n",
        "    cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell(\n",
        "        [tf.compat.v1.nn.rnn_cell.DropoutWrapper(\n",
        "            tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.lstm_size),\n",
        "            output_keep_prob=tf_keepprob)\n",
        "        for _ in range(self.num_layers)])\n",
        "    \n",
        "    #define the initial state\n",
        "    self.initial_state = cells.zero_state(batch_size, tf.float32)\n",
        "\n",
        "    #run each sequence step through the RNN\n",
        "    lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
        "        cells, x_onehot, initial_state=self.initial_state\n",
        "    )\n",
        "\n",
        "    print('<< lstm_outputs >>', lstm_outputs)\n",
        "\n",
        "    seq_output_reshaped = tf.reshape(\n",
        "        lstm_outputs, shape=[-1, self.lstm_size], name='seq_output_reshaped'\n",
        "    )       \n",
        "\n",
        "    logits = tf.layers.dense(\n",
        "        inputs=seq_output_reshaped, units = self.num_classes, activation=None, name='logits'\n",
        "    )\n",
        "\n",
        "    proba = tf.nn.softmax(\n",
        "        logits, name='probabilities'\n",
        "    )\n",
        "    print(proba)\n",
        "\n",
        "    y_reshaped = tf.reshape(y_onehot, shape=[-1, self.num_classes], name='y_reshaped')\n",
        "    cost = tf.reduce_mean(\n",
        "        tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=logits,\n",
        "            labels=y_reshaped\n",
        "        ),name='cost'\n",
        "        )\n",
        "    \n",
        "    #gradient clipping to avoid 'exploding gradients'\n",
        "    tvars = tf.trainable_variables()\n",
        "    grads, _ = tf.clip_by_global_norm(\n",
        "        tf.gradients(cost, tvars), \n",
        "        self.grad_clip\n",
        "    )\n",
        "    optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "    train_op = optimizer.apply_gradients(\n",
        "        zip(grads, tvars),\n",
        "        name='train_op'\n",
        "    )\n",
        "\n",
        "  def train(self, train_x, train_y, num_epochs, ckpt_dir='./model/'):\n",
        "    #create the checkpoint directory if does not exists\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "      os.mkdir(ckpt_dir)\n",
        "    \n",
        "    with tf.Session(graph=self.g) as sess:\n",
        "      sess.run(self.init_op)\n",
        "\n",
        "      n_batches = int(train_x.shape[1]/self.num_steps)\n",
        "      iterations = n_batches * num_epochs\n",
        "      for epoch in range(num_epochs):\n",
        "        #Train network\n",
        "        new_state = sess.run(self.initial_state)\n",
        "        loss = 0\n",
        "\n",
        "        #Minibatch generator\n",
        "        bgen = create_batch_generator(\n",
        "            train_x, train_y, self.num_steps\n",
        "        )\n",
        "        for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
        "          iteration = epoch*n_batches + b\n",
        "          \n",
        "          feed = {'tf_x:0':batch_x, \n",
        "                  'tf_y:0':batch_y,\n",
        "                  'tf_keepprob:0':self.keep_prob, \n",
        "                  self.initial_state:new_state}\n",
        "          batch_cost, _, new_state = sess.run(['cost:0', 'train_op', self.final_state], feed_dict=feed)\n",
        "          if iteration % 10 == 0:\n",
        "            print('Epoch %d/%d Iteration %d'\n",
        "                  '| Training loss: %.4f' % (\n",
        "                  epoch + 1, num_epochs, \n",
        "                  iteration, batch_cost))\n",
        "        \n",
        "        #save the trained model\n",
        "        self.saver.save(sess, os.path.join(ckpt_dir, 'language_modeling.ckpt'))\n",
        "\n",
        "  def sample(self, output_length, ckpt_dir, starter_seq='The '):\n",
        "    observed_seq = [ch for ch in starter_seq]\n",
        "    with tf.Session(graph=self.g) as sess:\n",
        "      self.saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
        "\n",
        "      #1. run the model using the starter sequence \n",
        "      new_state = sess.run(self.initial_state)\n",
        "      for ch in starter_seq:\n",
        "        x = np.zeros((1,1))\n",
        "        x[0, 0] = char2int[ch]\n",
        "        feed = {'tf_x:0':x, \n",
        "                'tf_keepprob':1.0, \n",
        "                self.initial_state:new_state}\n",
        "        proba, new_state = sess.run(['probabilities:0', self.final_state], feed_dict=feed)\n",
        "        ch_id = get_top_char(proba, len(chars))\n",
        "\n",
        "        #2. run the model using the updated observed_seq\n",
        "        for i in range(output_length):\n",
        "          x[0, 0] = ch_id\n",
        "          feed = {'tf_x:0':x, 'tf_keepprob:0':1.0, self.initial_state:new_state}\n",
        "          proba, new_state =sess.run(['probabilities:0', self.final_state], feed_dict = feed)\n",
        "          ch_id = get_top_char(proba, len(chars))\n",
        "          observed_seq.append(int2char[ch_id])\n",
        "    return ''.join(observed_seq)\n",
        "  \n",
        "def get_top_char(probas, char_size, top_n=5):\n",
        "  p = np.squence(probas)\n",
        "  p[np.argsort(p)[:-top_n]] = 0.0\n",
        "  p = p / np.sum(p)\n",
        "  ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
        "  return ch_id\n",
        "\n",
        "batch_size = 64\n",
        "num_steps = 100\n",
        "train_x, train_y = reshape_data(text_ints, batch_size, num_steps)\n",
        "\n",
        "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
        "rnn.train(train_x, train_y, num_epochs=100, ckpt_dir='./model-100/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWd_i0giSxm4",
        "outputId": "aa324a7f-b9d3-42d1-c1d6-73a826552640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-4-0035d807c343>:53: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/rnn/legacy_cells.py:726: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:63: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<< lstm_outputs >> Tensor(\"rnn/transpose_1:0\", shape=(64, 100, 128), dtype=float32)\n",
            "Tensor(\"probabilities:0\", shape=(6400, 96), dtype=float32)\n",
            "Epoch 1/100 Iteration 10| Training loss: 4.0818\n",
            "Epoch 1/100 Iteration 20| Training loss: 3.5514\n",
            "Epoch 1/100 Iteration 30| Training loss: 3.4178\n",
            "Epoch 1/100 Iteration 40| Training loss: 3.2891\n",
            "Epoch 1/100 Iteration 50| Training loss: 3.1327\n",
            "Epoch 1/100 Iteration 60| Training loss: 3.0266\n",
            "Epoch 1/100 Iteration 70| Training loss: 3.0153\n",
            "Epoch 1/100 Iteration 80| Training loss: 2.8987\n",
            "Epoch 1/100 Iteration 90| Training loss: 2.8205\n",
            "Epoch 1/100 Iteration 100| Training loss: 2.7154\n",
            "Epoch 1/100 Iteration 110| Training loss: 2.6061\n",
            "Epoch 1/100 Iteration 120| Training loss: 2.5500\n",
            "Epoch 1/100 Iteration 130| Training loss: 2.5002\n",
            "Epoch 1/100 Iteration 140| Training loss: 2.3152\n",
            "Epoch 1/100 Iteration 150| Training loss: 2.3309\n",
            "Epoch 1/100 Iteration 160| Training loss: 2.2595\n",
            "Epoch 1/100 Iteration 170| Training loss: 2.2029\n",
            "Epoch 1/100 Iteration 180| Training loss: 2.1111\n",
            "Epoch 1/100 Iteration 190| Training loss: 1.9998\n",
            "Epoch 1/100 Iteration 200| Training loss: 1.9679\n",
            "Epoch 1/100 Iteration 210| Training loss: 2.0580\n",
            "Epoch 1/100 Iteration 220| Training loss: 1.9212\n",
            "Epoch 1/100 Iteration 230| Training loss: 1.8446\n",
            "Epoch 2/100 Iteration 240| Training loss: 1.8187\n",
            "Epoch 2/100 Iteration 250| Training loss: 1.6891\n",
            "Epoch 2/100 Iteration 260| Training loss: 1.6793\n",
            "Epoch 2/100 Iteration 270| Training loss: 1.6304\n",
            "Epoch 2/100 Iteration 280| Training loss: 1.5472\n",
            "Epoch 2/100 Iteration 290| Training loss: 1.5714\n",
            "Epoch 2/100 Iteration 300| Training loss: 1.5179\n",
            "Epoch 2/100 Iteration 310| Training loss: 1.4635\n",
            "Epoch 2/100 Iteration 320| Training loss: 1.4315\n",
            "Epoch 2/100 Iteration 330| Training loss: 1.3490\n",
            "Epoch 2/100 Iteration 340| Training loss: 1.3402\n",
            "Epoch 2/100 Iteration 350| Training loss: 1.3670\n",
            "Epoch 2/100 Iteration 360| Training loss: 1.2528\n",
            "Epoch 2/100 Iteration 370| Training loss: 1.1668\n",
            "Epoch 2/100 Iteration 380| Training loss: 1.3145\n",
            "Epoch 2/100 Iteration 390| Training loss: 1.1755\n",
            "Epoch 2/100 Iteration 400| Training loss: 1.3163\n",
            "Epoch 2/100 Iteration 410| Training loss: 1.2891\n",
            "Epoch 2/100 Iteration 420| Training loss: 1.1296\n",
            "Epoch 2/100 Iteration 430| Training loss: 1.1487\n",
            "Epoch 2/100 Iteration 440| Training loss: 1.1330\n",
            "Epoch 2/100 Iteration 450| Training loss: 1.0522\n",
            "Epoch 2/100 Iteration 460| Training loss: 1.0517\n",
            "Epoch 2/100 Iteration 470| Training loss: 1.0443\n",
            "Epoch 3/100 Iteration 480| Training loss: 1.0514\n",
            "Epoch 3/100 Iteration 490| Training loss: 1.0608\n",
            "Epoch 3/100 Iteration 500| Training loss: 0.9543\n",
            "Epoch 3/100 Iteration 510| Training loss: 1.0424\n",
            "Epoch 3/100 Iteration 520| Training loss: 1.0673\n",
            "Epoch 3/100 Iteration 530| Training loss: 0.8861\n",
            "Epoch 3/100 Iteration 540| Training loss: 0.8588\n",
            "Epoch 3/100 Iteration 550| Training loss: 0.9073\n",
            "Epoch 3/100 Iteration 560| Training loss: 0.9645\n",
            "Epoch 3/100 Iteration 570| Training loss: 0.8603\n",
            "Epoch 3/100 Iteration 580| Training loss: 0.9257\n",
            "Epoch 3/100 Iteration 590| Training loss: 0.8914\n",
            "Epoch 3/100 Iteration 600| Training loss: 0.9021\n",
            "Epoch 3/100 Iteration 610| Training loss: 0.9148\n",
            "Epoch 3/100 Iteration 620| Training loss: 0.9414\n",
            "Epoch 3/100 Iteration 630| Training loss: 0.8879\n",
            "Epoch 3/100 Iteration 640| Training loss: 0.8969\n",
            "Epoch 3/100 Iteration 650| Training loss: 0.7185\n",
            "Epoch 3/100 Iteration 660| Training loss: 0.8743\n",
            "Epoch 3/100 Iteration 670| Training loss: 0.9525\n",
            "Epoch 3/100 Iteration 680| Training loss: 0.8823\n",
            "Epoch 3/100 Iteration 690| Training loss: 0.7479\n",
            "Epoch 3/100 Iteration 700| Training loss: 0.8826\n",
            "Epoch 3/100 Iteration 710| Training loss: 0.8159\n",
            "Epoch 4/100 Iteration 720| Training loss: 0.9313\n",
            "Epoch 4/100 Iteration 730| Training loss: 0.8001\n",
            "Epoch 4/100 Iteration 740| Training loss: 0.7030\n",
            "Epoch 4/100 Iteration 750| Training loss: 0.7196\n",
            "Epoch 4/100 Iteration 760| Training loss: 0.7229\n",
            "Epoch 4/100 Iteration 770| Training loss: 0.7577\n",
            "Epoch 4/100 Iteration 780| Training loss: 0.7316\n",
            "Epoch 4/100 Iteration 790| Training loss: 0.8418\n",
            "Epoch 4/100 Iteration 800| Training loss: 0.7172\n",
            "Epoch 4/100 Iteration 810| Training loss: 0.7921\n",
            "Epoch 4/100 Iteration 820| Training loss: 0.7540\n",
            "Epoch 4/100 Iteration 830| Training loss: 0.6894\n",
            "Epoch 4/100 Iteration 840| Training loss: 0.6511\n",
            "Epoch 4/100 Iteration 850| Training loss: 0.7414\n",
            "Epoch 4/100 Iteration 860| Training loss: 0.8580\n",
            "Epoch 4/100 Iteration 870| Training loss: 0.8019\n",
            "Epoch 4/100 Iteration 880| Training loss: 0.7456\n",
            "Epoch 4/100 Iteration 890| Training loss: 0.8463\n",
            "Epoch 4/100 Iteration 900| Training loss: 0.8277\n",
            "Epoch 4/100 Iteration 910| Training loss: 0.7848\n",
            "Epoch 4/100 Iteration 920| Training loss: 0.6342\n",
            "Epoch 4/100 Iteration 930| Training loss: 0.8982\n",
            "Epoch 4/100 Iteration 940| Training loss: 0.8549\n",
            "Epoch 4/100 Iteration 950| Training loss: 0.7915\n",
            "Epoch 5/100 Iteration 960| Training loss: 0.7628\n",
            "Epoch 5/100 Iteration 970| Training loss: 0.7119\n",
            "Epoch 5/100 Iteration 980| Training loss: 0.8482\n",
            "Epoch 5/100 Iteration 990| Training loss: 0.6707\n",
            "Epoch 5/100 Iteration 1000| Training loss: 0.6791\n",
            "Epoch 5/100 Iteration 1010| Training loss: 0.7044\n",
            "Epoch 5/100 Iteration 1020| Training loss: 0.7609\n",
            "Epoch 5/100 Iteration 1030| Training loss: 0.6696\n",
            "Epoch 5/100 Iteration 1040| Training loss: 0.6686\n",
            "Epoch 5/100 Iteration 1050| Training loss: 0.7050\n",
            "Epoch 5/100 Iteration 1060| Training loss: 0.7435\n",
            "Epoch 5/100 Iteration 1070| Training loss: 0.7131\n",
            "Epoch 5/100 Iteration 1080| Training loss: 0.6574\n",
            "Epoch 5/100 Iteration 1090| Training loss: 0.8292\n",
            "Epoch 5/100 Iteration 1100| Training loss: 0.7243\n",
            "Epoch 5/100 Iteration 1110| Training loss: 0.6148\n",
            "Epoch 5/100 Iteration 1120| Training loss: 0.7038\n",
            "Epoch 5/100 Iteration 1130| Training loss: 0.7652\n",
            "Epoch 5/100 Iteration 1140| Training loss: 0.7358\n",
            "Epoch 5/100 Iteration 1150| Training loss: 0.6444\n",
            "Epoch 5/100 Iteration 1160| Training loss: 0.6686\n",
            "Epoch 5/100 Iteration 1170| Training loss: 0.7076\n",
            "Epoch 5/100 Iteration 1180| Training loss: 0.6885\n",
            "Epoch 5/100 Iteration 1190| Training loss: 0.7390\n",
            "Epoch 6/100 Iteration 1200| Training loss: 0.6421\n",
            "Epoch 6/100 Iteration 1210| Training loss: 0.6539\n",
            "Epoch 6/100 Iteration 1220| Training loss: 0.6186\n",
            "Epoch 6/100 Iteration 1230| Training loss: 0.6343\n",
            "Epoch 6/100 Iteration 1240| Training loss: 0.6401\n",
            "Epoch 6/100 Iteration 1250| Training loss: 0.6663\n",
            "Epoch 6/100 Iteration 1260| Training loss: 0.6633\n",
            "Epoch 6/100 Iteration 1270| Training loss: 0.6182\n",
            "Epoch 6/100 Iteration 1280| Training loss: 0.6914\n",
            "Epoch 6/100 Iteration 1290| Training loss: 0.6369\n",
            "Epoch 6/100 Iteration 1300| Training loss: 0.6386\n",
            "Epoch 6/100 Iteration 1310| Training loss: 0.6954\n",
            "Epoch 6/100 Iteration 1320| Training loss: 0.8154\n",
            "Epoch 6/100 Iteration 1330| Training loss: 0.6784\n",
            "Epoch 6/100 Iteration 1340| Training loss: 0.6900\n",
            "Epoch 6/100 Iteration 1350| Training loss: 0.7090\n",
            "Epoch 6/100 Iteration 1360| Training loss: 0.8058\n",
            "Epoch 6/100 Iteration 1370| Training loss: 0.6877\n",
            "Epoch 6/100 Iteration 1380| Training loss: 0.6531\n",
            "Epoch 6/100 Iteration 1390| Training loss: 0.6444\n",
            "Epoch 6/100 Iteration 1400| Training loss: 0.8400\n",
            "Epoch 6/100 Iteration 1410| Training loss: 0.6831\n",
            "Epoch 6/100 Iteration 1420| Training loss: 0.7190\n",
            "Epoch 7/100 Iteration 1430| Training loss: 0.7094\n",
            "Epoch 7/100 Iteration 1440| Training loss: 0.6192\n",
            "Epoch 7/100 Iteration 1450| Training loss: 0.6765\n",
            "Epoch 7/100 Iteration 1460| Training loss: 0.6423\n",
            "Epoch 7/100 Iteration 1470| Training loss: 0.6401\n",
            "Epoch 7/100 Iteration 1480| Training loss: 0.7282\n",
            "Epoch 7/100 Iteration 1490| Training loss: 0.6922\n",
            "Epoch 7/100 Iteration 1500| Training loss: 0.6750\n",
            "Epoch 7/100 Iteration 1510| Training loss: 0.6505\n",
            "Epoch 7/100 Iteration 1520| Training loss: 0.6202\n",
            "Epoch 7/100 Iteration 1530| Training loss: 0.6425\n",
            "Epoch 7/100 Iteration 1540| Training loss: 0.7095\n",
            "Epoch 7/100 Iteration 1550| Training loss: 0.6010\n",
            "Epoch 7/100 Iteration 1560| Training loss: 0.5794\n",
            "Epoch 7/100 Iteration 1570| Training loss: 0.6996\n",
            "Epoch 7/100 Iteration 1580| Training loss: 0.5953\n",
            "Epoch 7/100 Iteration 1590| Training loss: 0.7835\n",
            "Epoch 7/100 Iteration 1600| Training loss: 0.7440\n",
            "Epoch 7/100 Iteration 1610| Training loss: 0.6452\n",
            "Epoch 7/100 Iteration 1620| Training loss: 0.6726\n",
            "Epoch 7/100 Iteration 1630| Training loss: 0.6831\n",
            "Epoch 7/100 Iteration 1640| Training loss: 0.6047\n",
            "Epoch 7/100 Iteration 1650| Training loss: 0.6364\n",
            "Epoch 7/100 Iteration 1660| Training loss: 0.6349\n",
            "Epoch 8/100 Iteration 1670| Training loss: 0.6436\n",
            "Epoch 8/100 Iteration 1680| Training loss: 0.6979\n",
            "Epoch 8/100 Iteration 1690| Training loss: 0.6077\n",
            "Epoch 8/100 Iteration 1700| Training loss: 0.6780\n",
            "Epoch 8/100 Iteration 1710| Training loss: 0.7145\n",
            "Epoch 8/100 Iteration 1720| Training loss: 0.5824\n",
            "Epoch 8/100 Iteration 1730| Training loss: 0.5599\n",
            "Epoch 8/100 Iteration 1740| Training loss: 0.6217\n",
            "Epoch 8/100 Iteration 1750| Training loss: 0.6746\n",
            "Epoch 8/100 Iteration 1760| Training loss: 0.5849\n",
            "Epoch 8/100 Iteration 1770| Training loss: 0.6319\n",
            "Epoch 8/100 Iteration 1780| Training loss: 0.6195\n",
            "Epoch 8/100 Iteration 1790| Training loss: 0.6390\n",
            "Epoch 8/100 Iteration 1800| Training loss: 0.6221\n",
            "Epoch 8/100 Iteration 1810| Training loss: 0.6916\n",
            "Epoch 8/100 Iteration 1820| Training loss: 0.6456\n",
            "Epoch 8/100 Iteration 1830| Training loss: 0.6683\n",
            "Epoch 8/100 Iteration 1840| Training loss: 0.5160\n",
            "Epoch 8/100 Iteration 1850| Training loss: 0.6355\n",
            "Epoch 8/100 Iteration 1860| Training loss: 0.6976\n",
            "Epoch 8/100 Iteration 1870| Training loss: 0.6862\n",
            "Epoch 8/100 Iteration 1880| Training loss: 0.5692\n",
            "Epoch 8/100 Iteration 1890| Training loss: 0.6825\n",
            "Epoch 8/100 Iteration 1900| Training loss: 0.6217\n",
            "Epoch 9/100 Iteration 1910| Training loss: 0.7188\n",
            "Epoch 9/100 Iteration 1920| Training loss: 0.6067\n",
            "Epoch 9/100 Iteration 1930| Training loss: 0.5350\n",
            "Epoch 9/100 Iteration 1940| Training loss: 0.5569\n",
            "Epoch 9/100 Iteration 1950| Training loss: 0.5509\n",
            "Epoch 9/100 Iteration 1960| Training loss: 0.5762\n",
            "Epoch 9/100 Iteration 1970| Training loss: 0.5726\n",
            "Epoch 9/100 Iteration 1980| Training loss: 0.6629\n",
            "Epoch 9/100 Iteration 1990| Training loss: 0.5604\n",
            "Epoch 9/100 Iteration 2000| Training loss: 0.6314\n",
            "Epoch 9/100 Iteration 2010| Training loss: 0.5955\n",
            "Epoch 9/100 Iteration 2020| Training loss: 0.5475\n",
            "Epoch 9/100 Iteration 2030| Training loss: 0.5180\n",
            "Epoch 9/100 Iteration 2040| Training loss: 0.5877\n",
            "Epoch 9/100 Iteration 2050| Training loss: 0.6955\n",
            "Epoch 9/100 Iteration 2060| Training loss: 0.6527\n",
            "Epoch 9/100 Iteration 2070| Training loss: 0.6063\n",
            "Epoch 9/100 Iteration 2080| Training loss: 0.6947\n",
            "Epoch 9/100 Iteration 2090| Training loss: 0.6815\n",
            "Epoch 9/100 Iteration 2100| Training loss: 0.6481\n",
            "Epoch 9/100 Iteration 2110| Training loss: 0.5073\n",
            "Epoch 9/100 Iteration 2120| Training loss: 0.7581\n",
            "Epoch 9/100 Iteration 2130| Training loss: 0.7073\n",
            "Epoch 9/100 Iteration 2140| Training loss: 0.6497\n",
            "Epoch 10/100 Iteration 2150| Training loss: 0.5815\n",
            "Epoch 10/100 Iteration 2160| Training loss: 0.5560\n",
            "Epoch 10/100 Iteration 2170| Training loss: 0.6917\n",
            "Epoch 10/100 Iteration 2180| Training loss: 0.5497\n",
            "Epoch 10/100 Iteration 2190| Training loss: 0.5657\n",
            "Epoch 10/100 Iteration 2200| Training loss: 0.5746\n",
            "Epoch 10/100 Iteration 2210| Training loss: 0.6533\n",
            "Epoch 10/100 Iteration 2220| Training loss: 0.5655\n",
            "Epoch 10/100 Iteration 2230| Training loss: 0.5524\n",
            "Epoch 10/100 Iteration 2240| Training loss: 0.5916\n",
            "Epoch 10/100 Iteration 2250| Training loss: 0.6289\n",
            "Epoch 10/100 Iteration 2260| Training loss: 0.6048\n",
            "Epoch 10/100 Iteration 2270| Training loss: 0.5590\n",
            "Epoch 10/100 Iteration 2280| Training loss: 0.7258\n",
            "Epoch 10/100 Iteration 2290| Training loss: 0.6147\n",
            "Epoch 10/100 Iteration 2300| Training loss: 0.5248\n",
            "Epoch 10/100 Iteration 2310| Training loss: 0.6033\n",
            "Epoch 10/100 Iteration 2320| Training loss: 0.6484\n",
            "Epoch 10/100 Iteration 2330| Training loss: 0.6248\n",
            "Epoch 10/100 Iteration 2340| Training loss: 0.5566\n",
            "Epoch 10/100 Iteration 2350| Training loss: 0.5740\n",
            "Epoch 10/100 Iteration 2360| Training loss: 0.6177\n",
            "Epoch 10/100 Iteration 2370| Training loss: 0.6000\n",
            "Epoch 10/100 Iteration 2380| Training loss: 0.6436\n",
            "Epoch 11/100 Iteration 2390| Training loss: 0.5148\n",
            "Epoch 11/100 Iteration 2400| Training loss: 0.5439\n",
            "Epoch 11/100 Iteration 2410| Training loss: 0.5123\n",
            "Epoch 11/100 Iteration 2420| Training loss: 0.5521\n",
            "Epoch 11/100 Iteration 2430| Training loss: 0.5525\n",
            "Epoch 11/100 Iteration 2440| Training loss: 0.5702\n",
            "Epoch 11/100 Iteration 2450| Training loss: 0.5860\n",
            "Epoch 11/100 Iteration 2460| Training loss: 0.5369\n",
            "Epoch 11/100 Iteration 2470| Training loss: 0.5919\n",
            "Epoch 11/100 Iteration 2480| Training loss: 0.5524\n",
            "Epoch 11/100 Iteration 2490| Training loss: 0.5499\n",
            "Epoch 11/100 Iteration 2500| Training loss: 0.6173\n",
            "Epoch 11/100 Iteration 2510| Training loss: 0.7116\n",
            "Epoch 11/100 Iteration 2520| Training loss: 0.5767\n",
            "Epoch 11/100 Iteration 2530| Training loss: 0.6063\n",
            "Epoch 11/100 Iteration 2540| Training loss: 0.6131\n",
            "Epoch 11/100 Iteration 2550| Training loss: 0.7102\n",
            "Epoch 11/100 Iteration 2560| Training loss: 0.6050\n",
            "Epoch 11/100 Iteration 2570| Training loss: 0.5760\n",
            "Epoch 11/100 Iteration 2580| Training loss: 0.5543\n",
            "Epoch 11/100 Iteration 2590| Training loss: 0.7481\n",
            "Epoch 11/100 Iteration 2600| Training loss: 0.6052\n",
            "Epoch 11/100 Iteration 2610| Training loss: 0.6364\n",
            "Epoch 12/100 Iteration 2620| Training loss: 0.6303\n",
            "Epoch 12/100 Iteration 2630| Training loss: 0.5525\n",
            "Epoch 12/100 Iteration 2640| Training loss: 0.5969\n",
            "Epoch 12/100 Iteration 2650| Training loss: 0.5663\n",
            "Epoch 12/100 Iteration 2660| Training loss: 0.5463\n",
            "Epoch 12/100 Iteration 2670| Training loss: 0.6364\n",
            "Epoch 12/100 Iteration 2680| Training loss: 0.5984\n",
            "Epoch 12/100 Iteration 2690| Training loss: 0.6024\n",
            "Epoch 12/100 Iteration 2700| Training loss: 0.5845\n",
            "Epoch 12/100 Iteration 2710| Training loss: 0.5404\n",
            "Epoch 12/100 Iteration 2720| Training loss: 0.5651\n",
            "Epoch 12/100 Iteration 2730| Training loss: 0.6405\n",
            "Epoch 12/100 Iteration 2740| Training loss: 0.5323\n",
            "Epoch 12/100 Iteration 2750| Training loss: 0.5154\n",
            "Epoch 12/100 Iteration 2760| Training loss: 0.6195\n",
            "Epoch 12/100 Iteration 2770| Training loss: 0.5053\n",
            "Epoch 12/100 Iteration 2780| Training loss: 0.7125\n",
            "Epoch 12/100 Iteration 2790| Training loss: 0.6691\n",
            "Epoch 12/100 Iteration 2800| Training loss: 0.5815\n",
            "Epoch 12/100 Iteration 2810| Training loss: 0.5934\n",
            "Epoch 12/100 Iteration 2820| Training loss: 0.6151\n",
            "Epoch 12/100 Iteration 2830| Training loss: 0.5377\n",
            "Epoch 12/100 Iteration 2840| Training loss: 0.5601\n",
            "Epoch 12/100 Iteration 2850| Training loss: 0.5674\n",
            "Epoch 13/100 Iteration 2860| Training loss: 0.6160\n",
            "Epoch 13/100 Iteration 2870| Training loss: 0.6311\n",
            "Epoch 13/100 Iteration 2880| Training loss: 0.5405\n",
            "Epoch 13/100 Iteration 2890| Training loss: 0.5949\n",
            "Epoch 13/100 Iteration 2900| Training loss: 0.6371\n",
            "Epoch 13/100 Iteration 2910| Training loss: 0.5218\n",
            "Epoch 13/100 Iteration 2920| Training loss: 0.4926\n",
            "Epoch 13/100 Iteration 2930| Training loss: 0.5617\n",
            "Epoch 13/100 Iteration 2940| Training loss: 0.6085\n",
            "Epoch 13/100 Iteration 2950| Training loss: 0.5238\n",
            "Epoch 13/100 Iteration 2960| Training loss: 0.5639\n",
            "Epoch 13/100 Iteration 2970| Training loss: 0.5513\n",
            "Epoch 13/100 Iteration 2980| Training loss: 0.5697\n",
            "Epoch 13/100 Iteration 2990| Training loss: 0.5464\n",
            "Epoch 13/100 Iteration 3000| Training loss: 0.6284\n",
            "Epoch 13/100 Iteration 3010| Training loss: 0.5731\n",
            "Epoch 13/100 Iteration 3020| Training loss: 0.6189\n",
            "Epoch 13/100 Iteration 3030| Training loss: 0.4583\n",
            "Epoch 13/100 Iteration 3040| Training loss: 0.5625\n",
            "Epoch 13/100 Iteration 3050| Training loss: 0.6119\n",
            "Epoch 13/100 Iteration 3060| Training loss: 0.5726\n",
            "Epoch 13/100 Iteration 3070| Training loss: 0.4720\n",
            "Epoch 13/100 Iteration 3080| Training loss: 0.5916\n",
            "Epoch 13/100 Iteration 3090| Training loss: 0.5333\n",
            "Epoch 14/100 Iteration 3100| Training loss: 0.6378\n",
            "Epoch 14/100 Iteration 3110| Training loss: 0.5483\n",
            "Epoch 14/100 Iteration 3120| Training loss: 0.4748\n",
            "Epoch 14/100 Iteration 3130| Training loss: 0.5028\n",
            "Epoch 14/100 Iteration 3140| Training loss: 0.4946\n",
            "Epoch 14/100 Iteration 3150| Training loss: 0.5122\n",
            "Epoch 14/100 Iteration 3160| Training loss: 0.5116\n",
            "Epoch 14/100 Iteration 3170| Training loss: 0.6007\n",
            "Epoch 14/100 Iteration 3180| Training loss: 0.5052\n",
            "Epoch 14/100 Iteration 3190| Training loss: 0.5712\n",
            "Epoch 14/100 Iteration 3200| Training loss: 0.5249\n",
            "Epoch 14/100 Iteration 3210| Training loss: 0.5044\n",
            "Epoch 14/100 Iteration 3220| Training loss: 0.4664\n",
            "Epoch 14/100 Iteration 3230| Training loss: 0.5244\n",
            "Epoch 14/100 Iteration 3240| Training loss: 0.6134\n",
            "Epoch 14/100 Iteration 3250| Training loss: 0.5826\n",
            "Epoch 14/100 Iteration 3260| Training loss: 0.5534\n",
            "Epoch 14/100 Iteration 3270| Training loss: 0.6351\n",
            "Epoch 14/100 Iteration 3280| Training loss: 0.6181\n",
            "Epoch 14/100 Iteration 3290| Training loss: 0.5897\n",
            "Epoch 14/100 Iteration 3300| Training loss: 0.4548\n",
            "Epoch 14/100 Iteration 3310| Training loss: 0.6941\n",
            "Epoch 14/100 Iteration 3320| Training loss: 0.6535\n",
            "Epoch 14/100 Iteration 3330| Training loss: 0.5916\n",
            "Epoch 15/100 Iteration 3340| Training loss: 0.6346\n",
            "Epoch 15/100 Iteration 3350| Training loss: 0.5268\n",
            "Epoch 15/100 Iteration 3360| Training loss: 0.6534\n",
            "Epoch 15/100 Iteration 3370| Training loss: 0.5150\n",
            "Epoch 15/100 Iteration 3380| Training loss: 0.5172\n",
            "Epoch 15/100 Iteration 3390| Training loss: 0.5378\n",
            "Epoch 15/100 Iteration 3400| Training loss: 0.6088\n",
            "Epoch 15/100 Iteration 3410| Training loss: 0.5156\n",
            "Epoch 15/100 Iteration 3420| Training loss: 0.5103\n",
            "Epoch 15/100 Iteration 3430| Training loss: 0.5345\n",
            "Epoch 15/100 Iteration 3440| Training loss: 0.5705\n",
            "Epoch 15/100 Iteration 3450| Training loss: 0.5674\n",
            "Epoch 15/100 Iteration 3460| Training loss: 0.5238\n",
            "Epoch 15/100 Iteration 3470| Training loss: 0.6784\n",
            "Epoch 15/100 Iteration 3480| Training loss: 0.5520\n",
            "Epoch 15/100 Iteration 3490| Training loss: 0.4799\n",
            "Epoch 15/100 Iteration 3500| Training loss: 0.5444\n",
            "Epoch 15/100 Iteration 3510| Training loss: 0.5863\n",
            "Epoch 15/100 Iteration 3520| Training loss: 0.5646\n",
            "Epoch 15/100 Iteration 3530| Training loss: 0.5240\n",
            "Epoch 15/100 Iteration 3540| Training loss: 0.5175\n",
            "Epoch 15/100 Iteration 3550| Training loss: 0.5777\n",
            "Epoch 15/100 Iteration 3560| Training loss: 0.5557\n",
            "Epoch 15/100 Iteration 3570| Training loss: 0.5990\n",
            "Epoch 16/100 Iteration 3580| Training loss: 0.4747\n",
            "Epoch 16/100 Iteration 3590| Training loss: 0.5002\n",
            "Epoch 16/100 Iteration 3600| Training loss: 0.4546\n",
            "Epoch 16/100 Iteration 3610| Training loss: 0.5174\n",
            "Epoch 16/100 Iteration 3620| Training loss: 0.5078\n",
            "Epoch 16/100 Iteration 3630| Training loss: 0.5298\n",
            "Epoch 16/100 Iteration 3640| Training loss: 0.5459\n",
            "Epoch 16/100 Iteration 3650| Training loss: 0.4962\n",
            "Epoch 16/100 Iteration 3660| Training loss: 0.5334\n",
            "Epoch 16/100 Iteration 3670| Training loss: 0.5060\n",
            "Epoch 16/100 Iteration 3680| Training loss: 0.5067\n",
            "Epoch 16/100 Iteration 3690| Training loss: 0.5703\n",
            "Epoch 16/100 Iteration 3700| Training loss: 0.6491\n",
            "Epoch 16/100 Iteration 3710| Training loss: 0.5346\n",
            "Epoch 16/100 Iteration 3720| Training loss: 0.5655\n",
            "Epoch 16/100 Iteration 3730| Training loss: 0.5605\n",
            "Epoch 16/100 Iteration 3740| Training loss: 0.6423\n",
            "Epoch 16/100 Iteration 3750| Training loss: 0.5549\n",
            "Epoch 16/100 Iteration 3760| Training loss: 0.5310\n",
            "Epoch 16/100 Iteration 3770| Training loss: 0.5005\n",
            "Epoch 16/100 Iteration 3780| Training loss: 0.6880\n",
            "Epoch 16/100 Iteration 3790| Training loss: 0.5527\n",
            "Epoch 16/100 Iteration 3800| Training loss: 0.6008\n",
            "Epoch 17/100 Iteration 3810| Training loss: 0.5907\n",
            "Epoch 17/100 Iteration 3820| Training loss: 0.5023\n",
            "Epoch 17/100 Iteration 3830| Training loss: 0.5380\n",
            "Epoch 17/100 Iteration 3840| Training loss: 0.5140\n",
            "Epoch 17/100 Iteration 3850| Training loss: 0.5051\n",
            "Epoch 17/100 Iteration 3860| Training loss: 0.5862\n",
            "Epoch 17/100 Iteration 3870| Training loss: 0.5549\n",
            "Epoch 17/100 Iteration 3880| Training loss: 0.5565\n",
            "Epoch 17/100 Iteration 3890| Training loss: 0.5308\n",
            "Epoch 17/100 Iteration 3900| Training loss: 0.4883\n",
            "Epoch 17/100 Iteration 3910| Training loss: 0.5237\n",
            "Epoch 17/100 Iteration 3920| Training loss: 0.5941\n",
            "Epoch 17/100 Iteration 3930| Training loss: 0.4977\n",
            "Epoch 17/100 Iteration 3940| Training loss: 0.4763\n",
            "Epoch 17/100 Iteration 3950| Training loss: 0.5782\n",
            "Epoch 17/100 Iteration 3960| Training loss: 0.4610\n",
            "Epoch 17/100 Iteration 3970| Training loss: 0.6757\n",
            "Epoch 17/100 Iteration 3980| Training loss: 0.6295\n",
            "Epoch 17/100 Iteration 3990| Training loss: 0.5430\n",
            "Epoch 17/100 Iteration 4000| Training loss: 0.5457\n",
            "Epoch 17/100 Iteration 4010| Training loss: 0.5634\n",
            "Epoch 17/100 Iteration 4020| Training loss: 0.4958\n",
            "Epoch 17/100 Iteration 4030| Training loss: 0.5153\n",
            "Epoch 17/100 Iteration 4040| Training loss: 0.5329\n",
            "Epoch 18/100 Iteration 4050| Training loss: 0.5048\n",
            "Epoch 18/100 Iteration 4060| Training loss: 0.5733\n",
            "Epoch 18/100 Iteration 4070| Training loss: 0.4916\n",
            "Epoch 18/100 Iteration 4080| Training loss: 0.5301\n",
            "Epoch 18/100 Iteration 4090| Training loss: 0.5834\n",
            "Epoch 18/100 Iteration 4100| Training loss: 0.4798\n",
            "Epoch 18/100 Iteration 4110| Training loss: 0.4416\n",
            "Epoch 18/100 Iteration 4120| Training loss: 0.5278\n",
            "Epoch 18/100 Iteration 4130| Training loss: 0.5648\n",
            "Epoch 18/100 Iteration 4140| Training loss: 0.4855\n",
            "Epoch 18/100 Iteration 4150| Training loss: 0.5269\n",
            "Epoch 18/100 Iteration 4160| Training loss: 0.5008\n",
            "Epoch 18/100 Iteration 4170| Training loss: 0.5283\n",
            "Epoch 18/100 Iteration 4180| Training loss: 0.4963\n",
            "Epoch 18/100 Iteration 4190| Training loss: 0.5822\n",
            "Epoch 18/100 Iteration 4200| Training loss: 0.5417\n",
            "Epoch 18/100 Iteration 4210| Training loss: 0.5790\n",
            "Epoch 18/100 Iteration 4220| Training loss: 0.4276\n",
            "Epoch 18/100 Iteration 4230| Training loss: 0.5259\n",
            "Epoch 18/100 Iteration 4240| Training loss: 0.5693\n",
            "Epoch 18/100 Iteration 4250| Training loss: 0.5226\n",
            "Epoch 18/100 Iteration 4260| Training loss: 0.4294\n",
            "Epoch 18/100 Iteration 4270| Training loss: 0.5416\n",
            "Epoch 18/100 Iteration 4280| Training loss: 0.4877\n",
            "Epoch 19/100 Iteration 4290| Training loss: 0.6176\n",
            "Epoch 19/100 Iteration 4300| Training loss: 0.5181\n",
            "Epoch 19/100 Iteration 4310| Training loss: 0.4465\n",
            "Epoch 19/100 Iteration 4320| Training loss: 0.4632\n",
            "Epoch 19/100 Iteration 4330| Training loss: 0.4568\n",
            "Epoch 19/100 Iteration 4340| Training loss: 0.4792\n",
            "Epoch 19/100 Iteration 4350| Training loss: 0.4774\n",
            "Epoch 19/100 Iteration 4360| Training loss: 0.5570\n",
            "Epoch 19/100 Iteration 4370| Training loss: 0.4765\n",
            "Epoch 19/100 Iteration 4380| Training loss: 0.5381\n",
            "Epoch 19/100 Iteration 4390| Training loss: 0.4866\n",
            "Epoch 19/100 Iteration 4400| Training loss: 0.4670\n",
            "Epoch 19/100 Iteration 4410| Training loss: 0.4377\n",
            "Epoch 19/100 Iteration 4420| Training loss: 0.4885\n",
            "Epoch 19/100 Iteration 4430| Training loss: 0.5733\n",
            "Epoch 19/100 Iteration 4440| Training loss: 0.5496\n",
            "Epoch 19/100 Iteration 4450| Training loss: 0.5135\n",
            "Epoch 19/100 Iteration 4460| Training loss: 0.5982\n",
            "Epoch 19/100 Iteration 4470| Training loss: 0.5705\n",
            "Epoch 19/100 Iteration 4480| Training loss: 0.5569\n",
            "Epoch 19/100 Iteration 4490| Training loss: 0.4147\n",
            "Epoch 19/100 Iteration 4500| Training loss: 0.6543\n",
            "Epoch 19/100 Iteration 4510| Training loss: 0.6102\n",
            "Epoch 19/100 Iteration 4520| Training loss: 0.5607\n",
            "Epoch 20/100 Iteration 4530| Training loss: 0.4652\n",
            "Epoch 20/100 Iteration 4540| Training loss: 0.4748\n",
            "Epoch 20/100 Iteration 4550| Training loss: 0.7232\n",
            "Epoch 20/100 Iteration 4560| Training loss: 0.5689\n",
            "Epoch 20/100 Iteration 4570| Training loss: 0.5449\n",
            "Epoch 20/100 Iteration 4580| Training loss: 0.5277\n",
            "Epoch 20/100 Iteration 4590| Training loss: 0.6030\n",
            "Epoch 20/100 Iteration 4600| Training loss: 0.4947\n",
            "Epoch 20/100 Iteration 4610| Training loss: 0.4753\n",
            "Epoch 20/100 Iteration 4620| Training loss: 0.5036\n",
            "Epoch 20/100 Iteration 4630| Training loss: 0.5451\n",
            "Epoch 20/100 Iteration 4640| Training loss: 0.5406\n",
            "Epoch 20/100 Iteration 4650| Training loss: 0.4898\n",
            "Epoch 20/100 Iteration 4660| Training loss: 0.6518\n",
            "Epoch 20/100 Iteration 4670| Training loss: 0.5221\n",
            "Epoch 20/100 Iteration 4680| Training loss: 0.4517\n",
            "Epoch 20/100 Iteration 4690| Training loss: 0.5113\n",
            "Epoch 20/100 Iteration 4700| Training loss: 0.5457\n",
            "Epoch 20/100 Iteration 4710| Training loss: 0.5297\n",
            "Epoch 20/100 Iteration 4720| Training loss: 0.4803\n",
            "Epoch 20/100 Iteration 4730| Training loss: 0.4885\n",
            "Epoch 20/100 Iteration 4740| Training loss: 0.5520\n",
            "Epoch 20/100 Iteration 4750| Training loss: 0.5263\n",
            "Epoch 20/100 Iteration 4760| Training loss: 0.5732\n",
            "Epoch 21/100 Iteration 4770| Training loss: 0.4495\n",
            "Epoch 21/100 Iteration 4780| Training loss: 0.4726\n",
            "Epoch 21/100 Iteration 4790| Training loss: 0.4143\n",
            "Epoch 21/100 Iteration 4800| Training loss: 0.4894\n",
            "Epoch 21/100 Iteration 4810| Training loss: 0.4627\n",
            "Epoch 21/100 Iteration 4820| Training loss: 0.4940\n",
            "Epoch 21/100 Iteration 4830| Training loss: 0.5289\n",
            "Epoch 21/100 Iteration 4840| Training loss: 0.4614\n",
            "Epoch 21/100 Iteration 4850| Training loss: 0.4989\n",
            "Epoch 21/100 Iteration 4860| Training loss: 0.4816\n",
            "Epoch 21/100 Iteration 4870| Training loss: 0.4734\n",
            "Epoch 21/100 Iteration 4880| Training loss: 0.5439\n",
            "Epoch 21/100 Iteration 4890| Training loss: 0.6052\n",
            "Epoch 21/100 Iteration 4900| Training loss: 0.5038\n",
            "Epoch 21/100 Iteration 4910| Training loss: 0.5358\n",
            "Epoch 21/100 Iteration 4920| Training loss: 0.5239\n",
            "Epoch 21/100 Iteration 4930| Training loss: 0.6087\n",
            "Epoch 21/100 Iteration 4940| Training loss: 0.5150\n",
            "Epoch 21/100 Iteration 4950| Training loss: 0.5019\n",
            "Epoch 21/100 Iteration 4960| Training loss: 0.4718\n",
            "Epoch 21/100 Iteration 4970| Training loss: 0.6501\n",
            "Epoch 21/100 Iteration 4980| Training loss: 0.5250\n",
            "Epoch 21/100 Iteration 4990| Training loss: 0.5579\n",
            "Epoch 22/100 Iteration 5000| Training loss: 0.5587\n",
            "Epoch 22/100 Iteration 5010| Training loss: 0.4668\n",
            "Epoch 22/100 Iteration 5020| Training loss: 0.5133\n",
            "Epoch 22/100 Iteration 5030| Training loss: 0.4746\n",
            "Epoch 22/100 Iteration 5040| Training loss: 0.4740\n",
            "Epoch 22/100 Iteration 5050| Training loss: 0.5549\n",
            "Epoch 22/100 Iteration 5060| Training loss: 0.5210\n",
            "Epoch 22/100 Iteration 5070| Training loss: 0.5384\n",
            "Epoch 22/100 Iteration 5080| Training loss: 0.5037\n",
            "Epoch 22/100 Iteration 5090| Training loss: 0.4587\n",
            "Epoch 22/100 Iteration 5100| Training loss: 0.4803\n",
            "Epoch 22/100 Iteration 5110| Training loss: 0.5623\n",
            "Epoch 22/100 Iteration 5120| Training loss: 0.4672\n",
            "Epoch 22/100 Iteration 5130| Training loss: 0.4504\n",
            "Epoch 22/100 Iteration 5140| Training loss: 0.5556\n",
            "Epoch 22/100 Iteration 5150| Training loss: 0.4280\n",
            "Epoch 22/100 Iteration 5160| Training loss: 0.6533\n",
            "Epoch 22/100 Iteration 5170| Training loss: 0.5965\n",
            "Epoch 22/100 Iteration 5180| Training loss: 0.5071\n",
            "Epoch 22/100 Iteration 5190| Training loss: 0.5135\n",
            "Epoch 22/100 Iteration 5200| Training loss: 0.5292\n",
            "Epoch 22/100 Iteration 5210| Training loss: 0.4673\n",
            "Epoch 22/100 Iteration 5220| Training loss: 0.4760\n",
            "Epoch 22/100 Iteration 5230| Training loss: 0.4969\n",
            "Epoch 23/100 Iteration 5240| Training loss: 0.4747\n",
            "Epoch 23/100 Iteration 5250| Training loss: 0.5442\n",
            "Epoch 23/100 Iteration 5260| Training loss: 0.4667\n",
            "Epoch 23/100 Iteration 5270| Training loss: 0.4990\n",
            "Epoch 23/100 Iteration 5280| Training loss: 0.5456\n",
            "Epoch 23/100 Iteration 5290| Training loss: 0.4527\n",
            "Epoch 23/100 Iteration 5300| Training loss: 0.4082\n",
            "Epoch 23/100 Iteration 5310| Training loss: 0.4936\n",
            "Epoch 23/100 Iteration 5320| Training loss: 0.5297\n",
            "Epoch 23/100 Iteration 5330| Training loss: 0.4569\n",
            "Epoch 23/100 Iteration 5340| Training loss: 0.4878\n",
            "Epoch 23/100 Iteration 5350| Training loss: 0.4671\n",
            "Epoch 23/100 Iteration 5360| Training loss: 0.5006\n",
            "Epoch 23/100 Iteration 5370| Training loss: 0.4666\n",
            "Epoch 23/100 Iteration 5380| Training loss: 0.5475\n",
            "Epoch 23/100 Iteration 5390| Training loss: 0.5035\n",
            "Epoch 23/100 Iteration 5400| Training loss: 0.5631\n",
            "Epoch 23/100 Iteration 5410| Training loss: 0.4009\n",
            "Epoch 23/100 Iteration 5420| Training loss: 0.4994\n",
            "Epoch 23/100 Iteration 5430| Training loss: 0.5332\n",
            "Epoch 23/100 Iteration 5440| Training loss: 0.4968\n",
            "Epoch 23/100 Iteration 5450| Training loss: 0.4060\n",
            "Epoch 23/100 Iteration 5460| Training loss: 0.5088\n",
            "Epoch 23/100 Iteration 5470| Training loss: 0.4561\n",
            "Epoch 24/100 Iteration 5480| Training loss: 0.5680\n",
            "Epoch 24/100 Iteration 5490| Training loss: 0.5095\n",
            "Epoch 24/100 Iteration 5500| Training loss: 0.4265\n",
            "Epoch 24/100 Iteration 5510| Training loss: 0.4310\n",
            "Epoch 24/100 Iteration 5520| Training loss: 0.4276\n",
            "Epoch 24/100 Iteration 5530| Training loss: 0.4507\n",
            "Epoch 24/100 Iteration 5540| Training loss: 0.4492\n",
            "Epoch 24/100 Iteration 5550| Training loss: 0.5293\n",
            "Epoch 24/100 Iteration 5560| Training loss: 0.4453\n",
            "Epoch 24/100 Iteration 5570| Training loss: 0.5101\n",
            "Epoch 24/100 Iteration 5580| Training loss: 0.4477\n",
            "Epoch 24/100 Iteration 5590| Training loss: 0.4455\n",
            "Epoch 24/100 Iteration 5600| Training loss: 0.4190\n",
            "Epoch 24/100 Iteration 5610| Training loss: 0.4541\n",
            "Epoch 24/100 Iteration 5620| Training loss: 0.5369\n",
            "Epoch 24/100 Iteration 5630| Training loss: 0.5157\n",
            "Epoch 24/100 Iteration 5640| Training loss: 0.4882\n",
            "Epoch 24/100 Iteration 5650| Training loss: 0.5707\n",
            "Epoch 24/100 Iteration 5660| Training loss: 0.5392\n",
            "Epoch 24/100 Iteration 5670| Training loss: 0.5311\n",
            "Epoch 24/100 Iteration 5680| Training loss: 0.3934\n",
            "Epoch 24/100 Iteration 5690| Training loss: 0.6268\n",
            "Epoch 24/100 Iteration 5700| Training loss: 0.5884\n",
            "Epoch 24/100 Iteration 5710| Training loss: 0.5282\n",
            "Epoch 25/100 Iteration 5720| Training loss: 0.4389\n",
            "Epoch 25/100 Iteration 5730| Training loss: 0.4327\n",
            "Epoch 25/100 Iteration 5740| Training loss: 0.5793\n",
            "Epoch 25/100 Iteration 5750| Training loss: 0.4465\n",
            "Epoch 25/100 Iteration 5760| Training loss: 0.4591\n",
            "Epoch 25/100 Iteration 5770| Training loss: 0.4772\n",
            "Epoch 25/100 Iteration 5780| Training loss: 0.5630\n",
            "Epoch 25/100 Iteration 5790| Training loss: 0.4522\n",
            "Epoch 25/100 Iteration 5800| Training loss: 0.4361\n",
            "Epoch 25/100 Iteration 5810| Training loss: 0.4701\n",
            "Epoch 25/100 Iteration 5820| Training loss: 0.5050\n",
            "Epoch 25/100 Iteration 5830| Training loss: 0.5108\n",
            "Epoch 25/100 Iteration 5840| Training loss: 0.4598\n",
            "Epoch 25/100 Iteration 5850| Training loss: 0.6204\n",
            "Epoch 25/100 Iteration 5860| Training loss: 0.4977\n",
            "Epoch 25/100 Iteration 5870| Training loss: 0.4211\n",
            "Epoch 25/100 Iteration 5880| Training loss: 0.4870\n",
            "Epoch 25/100 Iteration 5890| Training loss: 0.5157\n",
            "Epoch 25/100 Iteration 5900| Training loss: 0.5007\n",
            "Epoch 25/100 Iteration 5910| Training loss: 0.4550\n",
            "Epoch 25/100 Iteration 5920| Training loss: 0.4622\n",
            "Epoch 25/100 Iteration 5930| Training loss: 0.5165\n",
            "Epoch 25/100 Iteration 5940| Training loss: 0.5085\n",
            "Epoch 25/100 Iteration 5950| Training loss: 0.5382\n",
            "Epoch 26/100 Iteration 5960| Training loss: 0.4152\n",
            "Epoch 26/100 Iteration 5970| Training loss: 0.4387\n",
            "Epoch 26/100 Iteration 5980| Training loss: 0.3833\n",
            "Epoch 26/100 Iteration 5990| Training loss: 0.4721\n",
            "Epoch 26/100 Iteration 6000| Training loss: 0.4437\n",
            "Epoch 26/100 Iteration 6010| Training loss: 0.4654\n",
            "Epoch 26/100 Iteration 6020| Training loss: 0.5096\n",
            "Epoch 26/100 Iteration 6030| Training loss: 0.4340\n",
            "Epoch 26/100 Iteration 6040| Training loss: 0.4682\n",
            "Epoch 26/100 Iteration 6050| Training loss: 0.4426\n",
            "Epoch 26/100 Iteration 6060| Training loss: 0.4456\n",
            "Epoch 26/100 Iteration 6070| Training loss: 0.5313\n",
            "Epoch 26/100 Iteration 6080| Training loss: 0.5669\n",
            "Epoch 26/100 Iteration 6090| Training loss: 0.4868\n",
            "Epoch 26/100 Iteration 6100| Training loss: 0.5061\n",
            "Epoch 26/100 Iteration 6110| Training loss: 0.4995\n",
            "Epoch 26/100 Iteration 6120| Training loss: 0.5718\n",
            "Epoch 26/100 Iteration 6130| Training loss: 0.4904\n",
            "Epoch 26/100 Iteration 6140| Training loss: 0.4761\n",
            "Epoch 26/100 Iteration 6150| Training loss: 0.4484\n",
            "Epoch 26/100 Iteration 6160| Training loss: 0.6206\n",
            "Epoch 26/100 Iteration 6170| Training loss: 0.4989\n",
            "Epoch 26/100 Iteration 6180| Training loss: 0.5372\n",
            "Epoch 27/100 Iteration 6190| Training loss: 0.5319\n",
            "Epoch 27/100 Iteration 6200| Training loss: 0.4539\n",
            "Epoch 27/100 Iteration 6210| Training loss: 0.4964\n",
            "Epoch 27/100 Iteration 6220| Training loss: 0.4453\n",
            "Epoch 27/100 Iteration 6230| Training loss: 0.4516\n",
            "Epoch 27/100 Iteration 6240| Training loss: 0.5184\n",
            "Epoch 27/100 Iteration 6250| Training loss: 0.4959\n",
            "Epoch 27/100 Iteration 6260| Training loss: 0.5095\n",
            "Epoch 27/100 Iteration 6270| Training loss: 0.4873\n",
            "Epoch 27/100 Iteration 6280| Training loss: 0.4325\n",
            "Epoch 27/100 Iteration 6290| Training loss: 0.4453\n",
            "Epoch 27/100 Iteration 6300| Training loss: 0.5435\n",
            "Epoch 27/100 Iteration 6310| Training loss: 0.4506\n",
            "Epoch 27/100 Iteration 6320| Training loss: 0.4216\n",
            "Epoch 27/100 Iteration 6330| Training loss: 0.5270\n",
            "Epoch 27/100 Iteration 6340| Training loss: 0.4066\n",
            "Epoch 27/100 Iteration 6350| Training loss: 0.6238\n",
            "Epoch 27/100 Iteration 6360| Training loss: 0.5725\n",
            "Epoch 27/100 Iteration 6370| Training loss: 0.4825\n",
            "Epoch 27/100 Iteration 6380| Training loss: 0.4865\n",
            "Epoch 27/100 Iteration 6390| Training loss: 0.5021\n",
            "Epoch 27/100 Iteration 6400| Training loss: 0.4482\n",
            "Epoch 27/100 Iteration 6410| Training loss: 0.4459\n",
            "Epoch 27/100 Iteration 6420| Training loss: 0.4764\n",
            "Epoch 28/100 Iteration 6430| Training loss: 0.4593\n",
            "Epoch 28/100 Iteration 6440| Training loss: 0.5334\n",
            "Epoch 28/100 Iteration 6450| Training loss: 0.4499\n",
            "Epoch 28/100 Iteration 6460| Training loss: 0.4650\n",
            "Epoch 28/100 Iteration 6470| Training loss: 0.5137\n",
            "Epoch 28/100 Iteration 6480| Training loss: 0.4257\n",
            "Epoch 28/100 Iteration 6490| Training loss: 0.3832\n",
            "Epoch 28/100 Iteration 6500| Training loss: 0.4742\n",
            "Epoch 28/100 Iteration 6510| Training loss: 0.5121\n",
            "Epoch 28/100 Iteration 6520| Training loss: 0.4302\n",
            "Epoch 28/100 Iteration 6530| Training loss: 0.4605\n",
            "Epoch 28/100 Iteration 6540| Training loss: 0.4430\n",
            "Epoch 28/100 Iteration 6550| Training loss: 0.4803\n",
            "Epoch 28/100 Iteration 6560| Training loss: 0.4468\n",
            "Epoch 28/100 Iteration 6570| Training loss: 0.5249\n",
            "Epoch 28/100 Iteration 6580| Training loss: 0.4900\n",
            "Epoch 28/100 Iteration 6590| Training loss: 0.5496\n",
            "Epoch 28/100 Iteration 6600| Training loss: 0.3838\n",
            "Epoch 28/100 Iteration 6610| Training loss: 0.4804\n",
            "Epoch 28/100 Iteration 6620| Training loss: 0.5024\n",
            "Epoch 28/100 Iteration 6630| Training loss: 0.4728\n",
            "Epoch 28/100 Iteration 6640| Training loss: 0.3817\n",
            "Epoch 28/100 Iteration 6650| Training loss: 0.4798\n",
            "Epoch 28/100 Iteration 6660| Training loss: 0.4294\n",
            "Epoch 29/100 Iteration 6670| Training loss: 0.5483\n",
            "Epoch 29/100 Iteration 6680| Training loss: 0.4720\n",
            "Epoch 29/100 Iteration 6690| Training loss: 0.4064\n",
            "Epoch 29/100 Iteration 6700| Training loss: 0.4089\n",
            "Epoch 29/100 Iteration 6710| Training loss: 0.4090\n",
            "Epoch 29/100 Iteration 6720| Training loss: 0.4246\n",
            "Epoch 29/100 Iteration 6730| Training loss: 0.4308\n",
            "Epoch 29/100 Iteration 6740| Training loss: 0.4958\n",
            "Epoch 29/100 Iteration 6750| Training loss: 0.4301\n",
            "Epoch 29/100 Iteration 6760| Training loss: 0.4878\n",
            "Epoch 29/100 Iteration 6770| Training loss: 0.4237\n",
            "Epoch 29/100 Iteration 6780| Training loss: 0.4318\n",
            "Epoch 29/100 Iteration 6790| Training loss: 0.3998\n",
            "Epoch 29/100 Iteration 6800| Training loss: 0.4441\n",
            "Epoch 29/100 Iteration 6810| Training loss: 0.5138\n",
            "Epoch 29/100 Iteration 6820| Training loss: 0.4977\n",
            "Epoch 29/100 Iteration 6830| Training loss: 0.4641\n",
            "Epoch 29/100 Iteration 6840| Training loss: 0.5454\n",
            "Epoch 29/100 Iteration 6850| Training loss: 0.5104\n",
            "Epoch 29/100 Iteration 6860| Training loss: 0.5029\n",
            "Epoch 29/100 Iteration 6870| Training loss: 0.3708\n",
            "Epoch 29/100 Iteration 6880| Training loss: 0.6006\n",
            "Epoch 29/100 Iteration 6890| Training loss: 0.5648\n",
            "Epoch 29/100 Iteration 6900| Training loss: 0.5031\n",
            "Epoch 30/100 Iteration 6910| Training loss: 0.4162\n",
            "Epoch 30/100 Iteration 6920| Training loss: 0.4046\n",
            "Epoch 30/100 Iteration 6930| Training loss: 0.5611\n",
            "Epoch 30/100 Iteration 6940| Training loss: 0.4243\n",
            "Epoch 30/100 Iteration 6950| Training loss: 0.4391\n",
            "Epoch 30/100 Iteration 6960| Training loss: 0.4488\n",
            "Epoch 30/100 Iteration 6970| Training loss: 0.5455\n",
            "Epoch 30/100 Iteration 6980| Training loss: 0.4303\n",
            "Epoch 30/100 Iteration 6990| Training loss: 0.4099\n",
            "Epoch 30/100 Iteration 7000| Training loss: 0.4525\n",
            "Epoch 30/100 Iteration 7010| Training loss: 0.4902\n",
            "Epoch 30/100 Iteration 7020| Training loss: 0.4885\n",
            "Epoch 30/100 Iteration 7030| Training loss: 0.4470\n",
            "Epoch 30/100 Iteration 7040| Training loss: 0.6037\n",
            "Epoch 30/100 Iteration 7050| Training loss: 0.4772\n",
            "Epoch 30/100 Iteration 7060| Training loss: 0.4034\n",
            "Epoch 30/100 Iteration 7070| Training loss: 0.4649\n",
            "Epoch 30/100 Iteration 7080| Training loss: 0.4874\n",
            "Epoch 30/100 Iteration 7090| Training loss: 0.4870\n",
            "Epoch 30/100 Iteration 7100| Training loss: 0.4270\n",
            "Epoch 30/100 Iteration 7110| Training loss: 0.4368\n",
            "Epoch 30/100 Iteration 7120| Training loss: 0.5000\n",
            "Epoch 30/100 Iteration 7130| Training loss: 0.4921\n",
            "Epoch 30/100 Iteration 7140| Training loss: 0.5280\n",
            "Epoch 31/100 Iteration 7150| Training loss: 0.3842\n",
            "Epoch 31/100 Iteration 7160| Training loss: 0.4160\n",
            "Epoch 31/100 Iteration 7170| Training loss: 0.3551\n",
            "Epoch 31/100 Iteration 7180| Training loss: 0.4495\n",
            "Epoch 31/100 Iteration 7190| Training loss: 0.4194\n",
            "Epoch 31/100 Iteration 7200| Training loss: 0.4437\n",
            "Epoch 31/100 Iteration 7210| Training loss: 0.4838\n",
            "Epoch 31/100 Iteration 7220| Training loss: 0.4139\n",
            "Epoch 31/100 Iteration 7230| Training loss: 0.4485\n",
            "Epoch 31/100 Iteration 7240| Training loss: 0.4225\n",
            "Epoch 31/100 Iteration 7250| Training loss: 0.4211\n",
            "Epoch 31/100 Iteration 7260| Training loss: 0.5037\n",
            "Epoch 31/100 Iteration 7270| Training loss: 0.5408\n",
            "Epoch 31/100 Iteration 7280| Training loss: 0.4590\n",
            "Epoch 31/100 Iteration 7290| Training loss: 0.4844\n",
            "Epoch 31/100 Iteration 7300| Training loss: 0.4761\n",
            "Epoch 31/100 Iteration 7310| Training loss: 0.5539\n",
            "Epoch 31/100 Iteration 7320| Training loss: 0.4687\n",
            "Epoch 31/100 Iteration 7330| Training loss: 0.4589\n",
            "Epoch 31/100 Iteration 7340| Training loss: 0.4285\n",
            "Epoch 31/100 Iteration 7350| Training loss: 0.5926\n",
            "Epoch 31/100 Iteration 7360| Training loss: 0.4785\n",
            "Epoch 31/100 Iteration 7370| Training loss: 0.5300\n",
            "Epoch 32/100 Iteration 7380| Training loss: 0.5127\n",
            "Epoch 32/100 Iteration 7390| Training loss: 0.4380\n",
            "Epoch 32/100 Iteration 7400| Training loss: 0.4755\n",
            "Epoch 32/100 Iteration 7410| Training loss: 0.4257\n",
            "Epoch 32/100 Iteration 7420| Training loss: 0.4279\n",
            "Epoch 32/100 Iteration 7430| Training loss: 0.5024\n",
            "Epoch 32/100 Iteration 7440| Training loss: 0.4746\n",
            "Epoch 32/100 Iteration 7450| Training loss: 0.4976\n",
            "Epoch 32/100 Iteration 7460| Training loss: 0.4664\n",
            "Epoch 32/100 Iteration 7470| Training loss: 0.4073\n",
            "Epoch 32/100 Iteration 7480| Training loss: 0.4216\n",
            "Epoch 32/100 Iteration 7490| Training loss: 0.5108\n",
            "Epoch 32/100 Iteration 7500| Training loss: 0.4319\n",
            "Epoch 32/100 Iteration 7510| Training loss: 0.4036\n",
            "Epoch 32/100 Iteration 7520| Training loss: 0.5112\n",
            "Epoch 32/100 Iteration 7530| Training loss: 0.3892\n",
            "Epoch 32/100 Iteration 7540| Training loss: 0.5995\n",
            "Epoch 32/100 Iteration 7550| Training loss: 0.5504\n",
            "Epoch 32/100 Iteration 7560| Training loss: 0.4646\n",
            "Epoch 32/100 Iteration 7570| Training loss: 0.4725\n",
            "Epoch 32/100 Iteration 7580| Training loss: 0.4838\n",
            "Epoch 32/100 Iteration 7590| Training loss: 0.4325\n",
            "Epoch 32/100 Iteration 7600| Training loss: 0.4265\n",
            "Epoch 32/100 Iteration 7610| Training loss: 0.4575\n",
            "Epoch 33/100 Iteration 7620| Training loss: 0.4378\n",
            "Epoch 33/100 Iteration 7630| Training loss: 0.5253\n",
            "Epoch 33/100 Iteration 7640| Training loss: 0.4330\n",
            "Epoch 33/100 Iteration 7650| Training loss: 0.4436\n",
            "Epoch 33/100 Iteration 7660| Training loss: 0.4836\n",
            "Epoch 33/100 Iteration 7670| Training loss: 0.4088\n",
            "Epoch 33/100 Iteration 7680| Training loss: 0.3656\n",
            "Epoch 33/100 Iteration 7690| Training loss: 0.4445\n",
            "Epoch 33/100 Iteration 7700| Training loss: 0.4919\n",
            "Epoch 33/100 Iteration 7710| Training loss: 0.4112\n",
            "Epoch 33/100 Iteration 7720| Training loss: 0.4423\n",
            "Epoch 33/100 Iteration 7730| Training loss: 0.4267\n",
            "Epoch 33/100 Iteration 7740| Training loss: 0.4759\n",
            "Epoch 33/100 Iteration 7750| Training loss: 0.4253\n",
            "Epoch 33/100 Iteration 7760| Training loss: 0.5034\n",
            "Epoch 33/100 Iteration 7770| Training loss: 0.4778\n",
            "Epoch 33/100 Iteration 7780| Training loss: 0.5358\n",
            "Epoch 33/100 Iteration 7790| Training loss: 0.3689\n",
            "Epoch 33/100 Iteration 7800| Training loss: 0.4620\n",
            "Epoch 33/100 Iteration 7810| Training loss: 0.4887\n",
            "Epoch 33/100 Iteration 7820| Training loss: 0.4535\n",
            "Epoch 33/100 Iteration 7830| Training loss: 0.3750\n",
            "Epoch 33/100 Iteration 7840| Training loss: 0.4688\n",
            "Epoch 33/100 Iteration 7850| Training loss: 0.4072\n",
            "Epoch 34/100 Iteration 7860| Training loss: 0.5318\n",
            "Epoch 34/100 Iteration 7870| Training loss: 0.4588\n",
            "Epoch 34/100 Iteration 7880| Training loss: 0.3888\n",
            "Epoch 34/100 Iteration 7890| Training loss: 0.3913\n",
            "Epoch 34/100 Iteration 7900| Training loss: 0.3882\n",
            "Epoch 34/100 Iteration 7910| Training loss: 0.4076\n",
            "Epoch 34/100 Iteration 7920| Training loss: 0.4163\n",
            "Epoch 34/100 Iteration 7930| Training loss: 0.4795\n",
            "Epoch 34/100 Iteration 7940| Training loss: 0.4041\n",
            "Epoch 34/100 Iteration 7950| Training loss: 0.4661\n",
            "Epoch 34/100 Iteration 7960| Training loss: 0.4027\n",
            "Epoch 34/100 Iteration 7970| Training loss: 0.4137\n",
            "Epoch 34/100 Iteration 7980| Training loss: 0.3818\n",
            "Epoch 34/100 Iteration 7990| Training loss: 0.4252\n",
            "Epoch 34/100 Iteration 8000| Training loss: 0.4951\n",
            "Epoch 34/100 Iteration 8010| Training loss: 0.4799\n",
            "Epoch 34/100 Iteration 8020| Training loss: 0.4457\n",
            "Epoch 34/100 Iteration 8030| Training loss: 0.5213\n",
            "Epoch 34/100 Iteration 8040| Training loss: 0.4883\n",
            "Epoch 34/100 Iteration 8050| Training loss: 0.4799\n",
            "Epoch 34/100 Iteration 8060| Training loss: 0.3559\n",
            "Epoch 34/100 Iteration 8070| Training loss: 0.5755\n",
            "Epoch 34/100 Iteration 8080| Training loss: 0.5376\n",
            "Epoch 34/100 Iteration 8090| Training loss: 0.4925\n",
            "Epoch 35/100 Iteration 8100| Training loss: 0.5651\n",
            "Epoch 35/100 Iteration 8110| Training loss: 0.4359\n",
            "Epoch 35/100 Iteration 8120| Training loss: 0.5698\n",
            "Epoch 35/100 Iteration 8130| Training loss: 0.4281\n",
            "Epoch 35/100 Iteration 8140| Training loss: 0.4373\n",
            "Epoch 35/100 Iteration 8150| Training loss: 0.4477\n",
            "Epoch 35/100 Iteration 8160| Training loss: 0.5344\n",
            "Epoch 35/100 Iteration 8170| Training loss: 0.4179\n",
            "Epoch 35/100 Iteration 8180| Training loss: 0.4014\n",
            "Epoch 35/100 Iteration 8190| Training loss: 0.4404\n",
            "Epoch 35/100 Iteration 8200| Training loss: 0.4694\n",
            "Epoch 35/100 Iteration 8210| Training loss: 0.4712\n",
            "Epoch 35/100 Iteration 8220| Training loss: 0.4289\n",
            "Epoch 35/100 Iteration 8230| Training loss: 0.5855\n",
            "Epoch 35/100 Iteration 8240| Training loss: 0.4664\n",
            "Epoch 35/100 Iteration 8250| Training loss: 0.3968\n",
            "Epoch 35/100 Iteration 8260| Training loss: 0.4432\n",
            "Epoch 35/100 Iteration 8270| Training loss: 0.4700\n",
            "Epoch 35/100 Iteration 8280| Training loss: 0.4686\n",
            "Epoch 35/100 Iteration 8290| Training loss: 0.4138\n",
            "Epoch 35/100 Iteration 8300| Training loss: 0.4280\n",
            "Epoch 35/100 Iteration 8310| Training loss: 0.4832\n",
            "Epoch 35/100 Iteration 8320| Training loss: 0.4728\n",
            "Epoch 35/100 Iteration 8330| Training loss: 0.5120\n",
            "Epoch 36/100 Iteration 8340| Training loss: 0.3837\n",
            "Epoch 36/100 Iteration 8350| Training loss: 0.4021\n",
            "Epoch 36/100 Iteration 8360| Training loss: 0.3408\n",
            "Epoch 36/100 Iteration 8370| Training loss: 0.4335\n",
            "Epoch 36/100 Iteration 8380| Training loss: 0.4070\n",
            "Epoch 36/100 Iteration 8390| Training loss: 0.4290\n",
            "Epoch 36/100 Iteration 8400| Training loss: 0.4788\n",
            "Epoch 36/100 Iteration 8410| Training loss: 0.4023\n",
            "Epoch 36/100 Iteration 8420| Training loss: 0.4270\n",
            "Epoch 36/100 Iteration 8430| Training loss: 0.4115\n",
            "Epoch 36/100 Iteration 8440| Training loss: 0.4114\n",
            "Epoch 36/100 Iteration 8450| Training loss: 0.4992\n",
            "Epoch 36/100 Iteration 8460| Training loss: 0.5199\n",
            "Epoch 36/100 Iteration 8470| Training loss: 0.4468\n",
            "Epoch 36/100 Iteration 8480| Training loss: 0.4663\n",
            "Epoch 36/100 Iteration 8490| Training loss: 0.4660\n",
            "Epoch 36/100 Iteration 8500| Training loss: 0.5275\n",
            "Epoch 36/100 Iteration 8510| Training loss: 0.4583\n",
            "Epoch 36/100 Iteration 8520| Training loss: 0.4410\n",
            "Epoch 36/100 Iteration 8530| Training loss: 0.4184\n",
            "Epoch 36/100 Iteration 8540| Training loss: 0.5813\n",
            "Epoch 36/100 Iteration 8550| Training loss: 0.4680\n",
            "Epoch 36/100 Iteration 8560| Training loss: 0.4995\n",
            "Epoch 37/100 Iteration 8570| Training loss: 0.4980\n",
            "Epoch 37/100 Iteration 8580| Training loss: 0.4159\n",
            "Epoch 37/100 Iteration 8590| Training loss: 0.4560\n",
            "Epoch 37/100 Iteration 8600| Training loss: 0.4072\n",
            "Epoch 37/100 Iteration 8610| Training loss: 0.4188\n",
            "Epoch 37/100 Iteration 8620| Training loss: 0.4917\n",
            "Epoch 37/100 Iteration 8630| Training loss: 0.4548\n",
            "Epoch 37/100 Iteration 8640| Training loss: 0.4734\n",
            "Epoch 37/100 Iteration 8650| Training loss: 0.4495\n",
            "Epoch 37/100 Iteration 8660| Training loss: 0.3972\n",
            "Epoch 37/100 Iteration 8670| Training loss: 0.4073\n",
            "Epoch 37/100 Iteration 8680| Training loss: 0.5005\n",
            "Epoch 37/100 Iteration 8690| Training loss: 0.4263\n",
            "Epoch 37/100 Iteration 8700| Training loss: 0.3846\n",
            "Epoch 37/100 Iteration 8710| Training loss: 0.4886\n",
            "Epoch 37/100 Iteration 8720| Training loss: 0.3682\n",
            "Epoch 37/100 Iteration 8730| Training loss: 0.5836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get result\n",
        "del rnn\n",
        "\n",
        "np.random.seed(123)\n",
        "rnn = CharRNN(len(chars), sampling=True)\n",
        "\n",
        "print(rnn.sample(ckpt_dir='./model-100/', output_length=500))\n"
      ],
      "metadata": {
        "id": "D-4RAE6ojetY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}